### Kubernetes learning notes
***************************************

* What is kubernetes ?
=> kubernetes is a open source container orchestration tool designed to automate the deployment, scaling, and management of containerized applications in different deployment environments(physical/virtual/cloud/hybrid).

Containers -> containers are lightweight, portable unit that package an application and its dependencies together into a single unit.

Orchestration -> when you have many containers running across multiple machines, you need a way to manage them - starting, stopping, scaling, updating, and ensuring they stay healthy.

Key features of kubernetes:
********************************
1. Automated deployment and scaling : Automatically deploy and scale/descale application based on demand. Scaling can be done horizontally(increasing no. of nodes/pods/container) or vertically(increasing cpu/ memory)
2. Load balancing (network) : Distributes traffic across pods/containers to ensure reliability and performance.
3. Rolling Updates & Rollbacks :  update application without any downtime and can rollback if something goes wrong
4. Service discovery : Automatically assigns ip addresses and a single DNS name for a set of containers
5. Secret and configuration management : Manages sensitive information like passwords and configuration settings securely.


Basic Components of kubernetes :
************************************
1. Pod - Smallest unit of deployment, in which one or more containers keeps running.
2. Node : A machine(physical/virtual) that runs pods
3. Cluster: A set of nodes manages by k8s
4. Master(Control Plane): manages the cluster and makes global decisions(like scheduling)
5. Worker nodes : nodes which form k8s cluster where pod/container runs
6. kubelet : an agent that runs on each node which helps in the creation of the pod.

Kubernetes Architecture :
****************************

Kubernetes cluster consists of control plane( that we say as master node) and nodes (worker machines)

Control plane(Top layer): 
*****************************
-> Manages the overall cluster and makes global decisions

There are 4 components which runs on kubernetes master node :

1. API Server : It is heart of k8s cluster, all components(controllers, schedulers) interact with the cluster through the API server.It receives RESTful API calls and processes them and then stores the data in the etcd database(in key value pair format). 
    -> As a end user we interact with api server component of k8s master node, whatever kubectl command we fire, command is sent as REST API request to the k8s API server.
    -> Validation of command/manifest file
    -> Authentication and Authorization 
      * Verify the identity of users (authentication)
      * Checks if they have permission to perform the requested action (authorization)

2. etcd  :  is a database (nosql format) that holds all cluster data in key-value pair format. etcd is a distributed, consistent, and highly available key-value store.
   
      -> etcd is a highly available and consistent key-value store that helps in Kubernetes backing store for all the cluster data
      -> Stores all configuration data (Configmaps(non-sensitive data), secrets(encrypted sensitive data)), user info(user roles(rbac), user permissions), node info(node status,             node metrics, node labels, taints and conditions), pod info(pod status, pod metrics, pod lables, spec (configuration like containers, volumes and restart policies)),       
         Replicasets, Deployment, Daemonsets , app versions, no. of replicas, connectivity etc.
  
just adding below lines about etcd for understanding purpose :

    -> Distributes means : Runs across multiple nodes for fault tolerance and high availability.
    -> Strong Consistency - Guarantees that reads always return the most recent write.
    -> High Availability - Can tolerate node failures using leader election and quorum-based writes.

3. Controller manager : is a key component of the control plane that runs controller processes - background loops that monitor the cluster state and make changes to move the current state toward the desired state

-> A controller is a control loop, that watches the state of your cluster through API server and makes changes to ensure actual state matches the desired state.

-> Process that run controller to handle cluster background task
-> It is a daemon(server) that runs in a continuous loop and responsible for gathering information and sending it to API server
-> Responsible for changing the current state of cluster to its desired state

Key function of controller manager : 

The kube-controller manager runs multiple controller in a single process, here are the main ones
1. Node controller - monitors the health of the nodes and manages node lifecycle, mark node as NotReady if node become unresponsive. Evicts pods from failed nodes.
2. Replication controller - ensure the specifies number of pods replicas are always running. Creates or deletes pods as needed
3. Deployment controller - manages rolling updates and rollback for deployments, ensure the desired version of an app is running
4. Endpoint Controller: Manages endpoints for services.
5. Service Account Controller: Manages service accounts. Creates default service accounts for new namespaces. Manages API access tokens for service accounts.
6. Job & CronJob Controllers : Manages one-time and scheduled jobs.Ensures jobs complete successfully or are retried if they fail.

=> How It Works (Simplified Flow):
  -> You define a desired state (e.g., 3 replicas of a pod).
  -> The controller watches the current state via the API server.
  -> If the actual state differs (e.g., only 2 pods running), the controller takes action (e.g., creates a new pod).
  -> It keeps looping to maintain the desired state.

4. Scheduler : The Kubernetes Scheduler is a core component of the control plane responsible for assigning newly created Pods to Nodes in the cluster. It ensures that workloads are placed on the most suitable nodes based on resource availability, constraints, and policies.

 Key Responsibilities of the Scheduler
********************************************
1. Pod Placement
-> When a new Pod is created (but not yet assigned to a Node), the Scheduler Watches for unassigned Pods via the API Server.
-> Selects an appropriate Node for each Pod.
-> Updates the Pod’s specification with the chosen Node.

2. Filtering Nodes
-> The Scheduler first filters out Nodes that cannot run the Pod due to:
-> Insufficient CPU, memory, or other resources.
-> Node taints and tolerations.
-> Node selectors or affinity rules.
-> Volume or port conflicts.

3. Scoring Nodes
-> Among the remaining Nodes, it scores them based on:
-> Resource availability (e.g., least loaded).
-> Pod affinity/anti-affinity.
-> Data locality (e.g., for storage).
-> Custom scheduling policies.


Components of Worker node  :
****************************

1. Kubelet : 
-> an agent which runs on each worker nodes in k8s cluster
-> Helps in the creation of pods and ensures the container containers are running inside pod

2. Kube-proxy 
-> Handles all network traffic in k8s cluster
-> routes connection to specific/correct pod and also perform load balancing across pods
-> Kube-Proxy enables Pods to communicate with Services using a stable virtual IP (ClusterIP).

3. Container runtime 
-> The software responsible for running containers (e.g., Docker, containerd, CRI-O).
-> Kubelet uses it to start, stop, and manage containers inside Pods.
-> Responsible for pulling images, starting/stopping containers, and managing storage and networking.


Kubernetes Pod:
**********************
-> smallest unit of deployment in K8S cluster
-> Contains one or more running container
-> Each pod gets its own IP address (internal)
-> New IP address on pod re-creation

=> So in short, a pod does not have stable IP address, its ip address changes frequently on pod deletion/re-creation or restarts, It is not possible to access the application using POD IP everytime, thats where we need kubernetes services. so we need stable IP address to communicate with pods and other resources within cluster, this is where we need kubernets services which provides you a stable endpoint (ClusterIP, DNS name) to access these pods.

Why do we need Kubernetes Services ?
****************************************

1. Pods are empheral 
    -> Pods can be created and destroyed frequently
    -> Each pod gets a new IP address when rec-created
    -> Services provide a stable endpoint (ClusterIP, DNS name) to access these pods

2. Load balancing 
    -> Services distribute traffic across multiple pods(or replica)
    -> Ensures high availability and better resource utilization

3. Service discovery 
    -> Kubernetes assigns a DNS name to each service
    -> Other pods can discover and connect to the service using this name

Types of Kubernetes Services :
*************************************

1. ClusterIP
2. NodePort
3. LoadBalancer
4. Ingress


1. ClusterIP Service - 
    -> It is default service in k8s which is used to internally expose the resources within cluster. It exposes the service on an internal IP in the cluster
    -> This IP is not accessible from outside the cluster.
    -> It allows Pods to communicate with each other using a consistent endpoint, even if the underlying Pods change.

Sample YAML definition file for nginx service at ClusterIP:
--------------------------------------------------------------
apiVersion: v1
kind: Service
metadata:
  name: nginx-service
spec:
  selector:
    app: nginx
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
  type: ClusterIP

Samnple NGINX Deployment YAML :
---------------------------------

apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 2
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        ports:
        - containerPort: 80


2. NodePort Service :
------------------------
-> Expose resources externally (it has a range 30000-32767)
-> NodePort exposes the service on each Node’s IP at a static port (the NodePort)
-> You can contact the NodePort service from outside the cluster, by requesting <Node IP>:<Node Port>

-> It allows users to access a service from outside the cluster using the node’s IP address and the assigned port.
-> Kubernetes allocates a port from a predefined range (default: 30000–32767).
-> This port is open on every node in the cluster.
-> Traffic sent to this port on any node is forwarded to the corresponding service and then to the appropriate Pod.

Sample YAML definition for NOdePort service :
---------------------------------------------------

apiVersion: v1
kind: Service
metadata:
  name: nginx-nodeport-service
spec:
  selector:
    app: nginx
  type: NodePort
  ports:
    - port: 80
      targetPort: 80
      nodePort: 30080

When to Use NodePort:
-----------------------------
-> For development or testing environments.
-> When you don’t have a cloud load balancer.
-> When you want direct access to services from outside the cluster.

3. Load Balancer Service : A LoadBalancer service in Kubernetes is a type of service that exposes your application to the internet by provisioning an external load balancer 
-----------------------------
(typically provided by a cloud provider like AWS, Azure, or GCP).

-> Expose resources externally
-> This service will use or dynamically create an external load balancer like a cloud load balancer when running in the cloud. This load balancer gets a public IP address.
-> It forwards external traffic to the Kubernetes service, which then routes it to the appropriate Pods.
-> Cloud provider will provide a mechanism for routing the traffic to the services
-> we can create network load balancer in cloud . cost will be there.

Sample YAML definition file for nginx load balancer:
----------------------------------------------------------
apiVersion: v1
kind: Service
metadata:
  name: nginx-loadbalancer-service
spec:
  selector:
    app: nginx
  type: LoadBalancer
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80

External Name Service:
------------------------
An ExternalName service in Kubernetes is a special type of service that maps a service name to an external DNS name. It doesn’t create a traditional service proxy or load balancer—instead, it acts like a DNS alias within the cluster.

Sample YAML definition file for external Name 
---------------------------------------------------
apiVersion: v1
kind: Service
metadata:
  name: external-db
spec:
  type: ExternalName
  externalName: db.example.com


Ingress : is a service which supports path based routing 
**********

Ingress in Kubernetes is an API object that manages external access to services within a cluster, typically over HTTP and HTTPS. It acts as a smart router that controls how requests from outside the cluster are routed to internal services based on rules like hostnames and URL paths.

=> Why Do We Need Ingress?
*********************************
By default, Kubernetes services are only accessible:

    -> Internally via ClusterIP
    -> Externally via NodePort or LoadBalancer, which are limited and not flexible

Ingress solves this by:

    -> Consolidating access under a single IP or domain
    -> Allowing path-based or host-based routing
    -> Supporting TLS termination (HTTPS)
    -> Enabling custom routing rules and authentication

How path based routing works ?
*************************************
1. Ingress Controller 
    -> Listens for incoming HTTP/HTTPS traffic from outside the cluster
    -> responsible for routing external HTTP(S) traffic to the appropriate services inside the Kubernetes cluster.

Popular Ingress Controllers :
********************************
NGINX - Most widely used, flexible and well-documented.
Traefik - Dynamic configuration, good for microservices.
HAProxy - High-performance, enterprise-grade.
Istio Gateway - Part of the Istio service mesh.
AWS ALB Ingress - Integrates with AWS Application Load Balancer.

2. Ingress resource 
    -> Define routing rules based on URL paths (e.g. /api/, /web, /admin)

3. Path rules
    -> Requests to /api are routed to Backend Service A.
    -> Requests to /web go to Backend Service B.
    -> Requests to /admin are sent to Backend Service C.

4. Backend Services
    -> Each service forwards traffic to its associated Pods (e.g., Pod A1, Pod A2).

Benefits of path based routing:
*************************************
-> Efficiently route different parts of an application (e.g. frontend, backend, admin) through a single domain
-> Simplifies traffic management and scaling.
-> Enables microservices architecture with clean separation.

Here's a sample Ingress YAML configuration for path-based routing in Kubernetes:

apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: example-ingress
  namespace: default
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - host: myapp.example.com
    http:
      paths:
      - path: /api
        pathType: Prefix
        backend:
          service:
            name: backend-service-a
            port:
              number: 80
      - path: /web
        pathType: Prefix
        backend:
          service:
            name: backend-service-b
            port:
              number: 80
      - path: /admin
        pathType: Prefix
        backend:
          service:
            name: backend-service-c
            port:
              number: 80

Explanation :
*****************
host: The domain name (e.g., myapp.example.com) that routes traffic through this Ingress.
paths: Each path (like /api, /web, /admin) is routed to a different backend service.
rewrite-target: Ensures the path is rewritten correctly when passed to the backend (optional, depends on your use case and ingress controller).



Kubernetes Deployment :
******************************
-> A deployment is an object in Kubernetes that lets you manage a set of identical pods
-> Ensure that a certain no. of replicas of pods are always running
-> Easily autoscale your applications using a Kubernetes deployment  (kubectl scale deployment <deployment-name> --replicas=<number-of-replicas> -n <ns>)
-> Rolling update & Roll back we can do in case of Deployment
-> Stateless applications are deployed using Deployment in K8S

-> Creates and manages pods automatically
-> Scale the number of replicas up or down
-> does rolling update and rollback

How it works ?
-------------------
1. You define a Deployment in a YAML file (or via kubectl).
2. Kubernetes creates a ReplicaSet based on the Deployment spec.
3. The ReplicaSet ensures the desired number of Pods are running.
4. If you update the Deployment(rolling update) (e.g., change the image), then Kubernetes:
    -> Creates a new ReplicaSet.
    -> Gradually shifts traffic from the old Pods to the new ones (rolling update).
    -> Deletes the old Pods once the new ones are ready.

Deployment Rolling update and Rollback commands :
*********************************************************
-> A rolling update allows you to update the Deployment (e.g., change the container image) without downtime.

# kubectl set image deployment/nginx-deployment nginx=nginx:1.21.0   (nginx image version is udpated to nginx:1.21.0)

-> This updates the container image in the Deployment to nginx:1.21.0.
-> Kubernetes will gradually replace old Pods with new ones.

Rollback to Previous Version :
------------------------------------

# kubectl rollout undo deployment/nginx-deployment

=> This reverts the Deployment to the last known good configuration.

Check rollout status :
--------------------------
# kubectl rollout status deployment/nginx-deployment

View Revision History :
------------------------------
# kubectl rollout history deployment/nginx-deployment



Sample deployment manifest yaml file :
-------------------------------------------
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        ports:
        - containerPort: 80


StatefulSet :
---------------
A StatefulSet is a Kubernetes workload API object used to manage stateful applications
Unlike Deployments, which are designed for stateless applications, StatefulSets are used when each Pod needs a unique identity, stable storage, and ordered deployment or scaling.

Key Features of StatefulSet:
--------------------------------
1. Stable Network Identity

    -> Each Pod gets a unique, stable DNS name (e.g., pod-0, pod-1, etc.).
    -> Useful for applications like databases that require fixed hostnames.

2. Stable Persistent Storage

    -> Each Pod can be associated with a PersistentVolumeClaim (PVC) that is not deleted when the Pod is rescheduled.
    -> Ensures data is preserved across restarts.

3. Ordered Deployment and Scaling

    -> Pods are created, updated, and deleted in order (e.g., pod-0 before pod-1).
    -> Ensures predictable behavior for clustered applications.

4. Graceful Rolling Updates
    -> Updates are done one Pod at a time, maintaining order and stability.

How It Works? :
----------------
When you create a StatefulSet with 3 replicas:
    -> Kubernetes creates Pods named myapp-0, myapp-1, and myapp-2.
    -> Each Pod gets its own PVC (e.g., myapp-pvc-0, myapp-pvc-1, etc.).
    -> If a Pod is deleted, it is recreated with the same name and volume.

Deployment and Statefulset difference :
*********************************************

1. Pod identity : in case of deployment pod identity is anonymous(random string at the end of each pod name), but in case of statefulset, pod identity would be unique or stable like(pod-0, pod-1, pod-2)

2. Storage : in case of deployment storage(pvc) is shared or ephemeral, but in case of statefulset storage(pvc) is dedicated for each pod (persistent per pod)

3. Use case - stateless application((e.g., web servers, rest api, microservices)) are deployed using deployment and statefulset applications(e.g., databases(mongodb, timescaledb, mysql), Kafka, rabbitmq) are deployed using statefulset

4. Pod management - in case of deployment it is parallel but in case of statefulset it is ordered (sequential)

5. DNS - shared service name in case of deployment but in case of statefulset it is Unique DNS per Pod.

Sample manifest yaml file for TimescaleDB statefulset :
---------------------------------------------------------------

apiVersion: v1
kind: Service
metadata:
  name: timescaledb-headless
  labels:
    app: timescaledb
spec:
  ports:
    - port: 5432
  clusterIP: None
  selector:
    app: timescaledb
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: timescaledb
spec:
  selector:
    matchLabels:
      app: timescaledb
  serviceName: "timescaledb-headless"
  replicas: 1
  template:
    metadata:
      labels:
        app: timescaledb
    spec:
      containers:
      - name: timescaledb
        image: timescale/timescaledb:latest-pg12
        ports:
        - containerPort: 5432
          name: postgres
        env:
        - name: POSTGRES_PASSWORD
          value: "yourpassword"
        volumeMounts:
        - name: timescale-storage
          mountPath: /var/lib/postgresql/data
  volumeClaimTemplates:
  - metadata:
      name: timescale-storage
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: 1Gi


What is a DaemonSet?
***************************
-> A DaemonSet ensures that a copy of a Pod runs on all (or some) nodes in the cluster. It’s typically used for background system-level tasks.

Common Use Cases:
---------------------
-> Log collection agents (e.g., Fluentd, Logstash)
-> Monitoring agents (e.g., Prometheus Node Exporter)
-> Network plugins (e.g., Calico, Cilium)
-> Storage daemons (e.g., GlusterFS, Ceph)

Sample YAML definition file for Daemonset:

apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: log-agent
spec:
  selector:
    matchLabels:
      name: log-agent
  template:
    metadata:
      labels:
        name: log-agent
    spec:
      containers:
      - name: log-agent
        image: fluentd



ConfigMap in k8s :
---------------------
-> In Kubernetes (K8s), a ConfigMap is an API object used to store non-confidential configuration data in key-value pairs.

Why Use ConfigMaps?
-----------------------
-> To externalize configuration from your application code.
-> To reuse the same container image in different environments (e.g., dev, staging, prod) with different configurations.
-> To update configuration without rebuilding your container image.

-> Environment Variables: ConfigMap values are injected into the container as environment variables.
-> Volume Mount: ConfigMap values are mounted as files inside the container (e.g., /etc/config).

Key Features
------------------
-> Stores data as key-value pairs.
-> Can be consumed by Pods as:
-> Environment variables
-> Command-line arguments
-> Configuration files in a volume

Sample YAMl file for configmap
-----------------------------------
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-config
data:
  APP_MODE: "production"
  LOG_LEVEL: "info"


Using ConfigMap in a Pod
-------------------------------

apiVersion: v1
kind: Pod
metadata:
  name: my-app
spec:
  containers:
  - name: app-container
    image: my-app-image
    env:
    - name: APP_MODE
      valueFrom:
        configMapKeyRef:
          name: my-config
          key: APP_MODE


Secrets :
************

In Kubernetes, a Secret is an object used to store sensitive data, such as passwords, OAuth tokens, SSH keys, or TLS certificates. Unlike ConfigMaps, Secrets are base64-encoded and can be configured to be more securely handled by the system.

Why Use Secrets?
-------------------
-> To protect sensitive information from being exposed in plain text.
-> To separate sensitive data from application code and configuration.
-> To control access to sensitive data using Kubernetes RBAC.

Sample YAMl file for secrets :
----------------------------------

apiVersion: v1
kind: Secret
metadata:
  name: my-secret
type: Opaque
data:
  username: YWRtaW4=        # base64 for "admin"
  password: c2VjcmV0MTIz    # base64 for "secret123"


###########

You can encode values using:
# echo -n 'admin' | base64

Using Secrets in a Pod:
-----------------------------

apiVersion: v1
kind: Pod
metadata:
  name: secret-env-pod
spec:
  containers:
  - name: my-container
    image: my-app-image
    env:
    - name: USERNAME
      valueFrom:
        secretKeyRef:
          name: my-secret
          key: username
    - name: PASSWORD
      valueFrom:
        secretKeyRef:
          name: my-secret
          key: password

As Mounted Files:
**********************

apiVersion: v1
kind: Pod
metadata:
  name: secret-volume-pod
spec:
  containers:
  - name: my-container
    image: my-app-image
    volumeMounts:
    - name: secret-volume
      mountPath: "/etc/secret-data"
      readOnly: true
  volumes:
  - name: secret-volume
    secret:
      secretName: my-secret

This will mount the secret keys as files under /etc/secret-data/username and /etc/secret-data/password.

How it works ?
--------------------
-> Secret: Stores sensitive data like username and password.
-> Pod: The application container that consumes the secret.
-> Environment Variable: The secret is injected into the container as environment variables.
-> Volume Mount: The secret is mounted as files inside the container (e.g., /etc/secret-data/username).
























