### Kubernetes learning notes
***************************************

* What is kubernetes ?
=> kubernetes is a open source container orchestration tool designed to automate the deployment, scaling, and management of containerized applications in different deployment environments(physical/virtual/cloud/hybrid).

Containers -> containers are lightweight, portable unit that package an application and its dependencies together into a single unit.

Orchestration -> when you have many containers running across multiple machines, you need a way to manage them - starting, stopping, scaling, updating, and ensuring they stay healthy.

Key features of kubernetes:
********************************
1. Automated deployment and scaling : Automatically deploy and scale/descale application based on demand. Scaling can be done horizontally(increasing no. of nodes/pods/container) or vertically(increasing cpu/ memory)
2. Load balancing (network) : Distributes traffic across pods/containers to ensure reliability and performance.
3. Rolling Updates & Rollbacks :  update application without any downtime and can rollback if something goes wrong
4. Service discovery : Automatically assigns ip addresses and a single DNS name for a set of containers
5. Secret and configuration management : Manages sensitive information like passwords and configuration settings securely.


Basic Components of kubernetes :
************************************
1. Pod - Smallest unit of deployment, in which one or more containers keeps running.
2. Node : A machine(physical/virtual) that runs pods
3. Cluster: A set of nodes manages by k8s
4. Master(Control Plane): manages the cluster and makes global decisions(like scheduling)
5. Worker nodes : nodes which form k8s cluster where pod/container runs
6. kubelet : an agent that runs on each node which helps in the creation of the pod.

Kubernetes Architecture :
****************************

Kubernetes cluster consists of control plane( that we say as master node) and nodes (worker machines)

Control plane(Top layer): 
*****************************
-> Manages the overall cluster and makes global decisions

There are 4 components which runs on kubernetes master node :

1. API Server : It is heart of k8s cluster, all components(controllers, schedulers) interact with the cluster through the API server.It receives RESTful API calls and processes them and then stores the data in the etcd database(in key value pair format). 
    -> As a end user we interact with api server component of k8s master node, whatever kubectl command we fire, command is sent as REST API request to the k8s API server.
    -> Validation of command/manifest file
    -> Authentication and Authorization 
      * Verify the identity of users (authentication)
      * Checks if they have permission to perform the requested action (authorization)

2. etcd  :  is a database (nosql format) that holds all cluster data in key-value pair format. etcd is a distributed, consistent, and highly available key-value store.
   
      -> etcd is a highly available and consistent key-value store that helps in Kubernetes backing store for all the cluster data
      -> Stores all configuration data (Configmaps(non-sensitive data), secrets(encrypted sensitive data)), user info(user roles(rbac), user permissions), node info(node status,             node metrics, node labels, taints and conditions), pod info(pod status, pod metrics, pod lables, spec (configuration like containers, volumes and restart policies)),       
         Replicasets, Deployment, Daemonsets , app versions, no. of replicas, connectivity etc.
  
just adding below lines about etcd for understanding purpose :

    -> Distributes means : Runs across multiple nodes for fault tolerance and high availability.
    -> Strong Consistency - Guarantees that reads always return the most recent write.
    -> High Availability - Can tolerate node failures using leader election and quorum-based writes.

3. Controller manager : is a key component of the control plane that runs controller processes - background loops that monitor the cluster state and make changes to move the current state toward the desired state

-> A controller is a control loop, that watches the state of your cluster through API server and makes changes to ensure actual state matches the desired state.

-> Process that run controller to handle cluster background task
-> It is a daemon(server) that runs in a continuous loop and responsible for gathering information and sending it to API server
-> Responsible for changing the current state of cluster to its desired state

Key function of controller manager : 

The kube-controller manager runs multiple controller in a single process, here are the main ones
1. Node controller - monitors the health of the nodes and manages node lifecycle, mark node as NotReady if node become unresponsive. Evicts pods from failed nodes.
2. Replication controller - ensure the specifies number of pods replicas are always running. Creates or deletes pods as needed
3. Deployment controller - manages rolling updates and rollback for deployments, ensure the desired version of an app is running
4. Endpoint Controller: Manages endpoints for services.
5. Service Account Controller: Manages service accounts. Creates default service accounts for new namespaces. Manages API access tokens for service accounts.
6. Job & CronJob Controllers : Manages one-time and scheduled jobs.Ensures jobs complete successfully or are retried if they fail.

=> How It Works (Simplified Flow):
  -> You define a desired state (e.g., 3 replicas of a pod).
  -> The controller watches the current state via the API server.
  -> If the actual state differs (e.g., only 2 pods running), the controller takes action (e.g., creates a new pod).
  -> It keeps looping to maintain the desired state.

4. Scheduler : The Kubernetes Scheduler is a core component of the control plane responsible for assigning newly created Pods to Nodes in the cluster. It ensures that workloads are placed on the most suitable nodes based on resource availability, constraints, and policies.

 Key Responsibilities of the Scheduler
********************************************
1. Pod Placement
-> When a new Pod is created (but not yet assigned to a Node), the Scheduler Watches for unassigned Pods via the API Server.
-> Selects an appropriate Node for each Pod.
-> Updates the Pod’s specification with the chosen Node.

2. Filtering Nodes
-> The Scheduler first filters out Nodes that cannot run the Pod due to:
-> Insufficient CPU, memory, or other resources.
-> Node taints and tolerations.
-> Node selectors or affinity rules.
-> Volume or port conflicts.

3. Scoring Nodes
-> Among the remaining Nodes, it scores them based on:
-> Resource availability (e.g., least loaded).
-> Pod affinity/anti-affinity.
-> Data locality (e.g., for storage).
-> Custom scheduling policies.


Components of Worker node  :
****************************

1. Kubelet : 
-> an agent which runs on each worker nodes in k8s cluster
-> Helps in the creation of pods and ensures the container containers are running inside pod

2. Kube-proxy 
-> Handles all network traffic in k8s cluster
-> routes connection to specific/correct pod and also perform load balancing across pods
-> Kube-Proxy enables Pods to communicate with Services using a stable virtual IP (ClusterIP).

3. Container runtime 
-> The software responsible for running containers (e.g., Docker, containerd, CRI-O).
-> Kubelet uses it to start, stop, and manage containers inside Pods.
-> Responsible for pulling images, starting/stopping containers, and managing storage and networking.


Kubernetes Pod:
**********************
-> smallest unit of deployment in K8S cluster
-> Contains one or more running container
-> Each pod gets its own IP address (internal)
-> New IP address on pod re-creation

=> So in short, a pod does not have stable IP address, its ip address changes frequently on pod deletion/re-creation or restarts, It is not possible to access the application using POD IP everytime, thats where we need kubernetes services. so we need stable IP address to communicate with pods and other resources within cluster, this is where we need kubernets services which provides you a stable endpoint (ClusterIP, DNS name) to access these pods.

Why do we need Kubernetes Services ?
****************************************

1. Pods are empheral 
    -> Pods can be created and destroyed frequently
    -> Each pod gets a new IP address when rec-created
    -> Services provide a stable endpoint (ClusterIP, DNS name) to access these pods

2. Load balancing 
    -> Services distribute traffic across multiple pods(or replica)
    -> Ensures high availability and better resource utilization

3. Service discovery 
    -> Kubernetes assigns a DNS name to each service
    -> Other pods can discover and connect to the service using this name

Types of Kubernetes Services :
*************************************

1. ClusterIP
2. NodePort
3. LoadBalancer
4. Ingress


1. ClusterIP Service - 
    -> It is default service in k8s which is used to internally expose the resources within cluster. It exposes the service on an internal IP in the cluster
    -> This IP is not accessible from outside the cluster.
    -> It allows Pods to communicate with each other using a consistent endpoint, even if the underlying Pods change.

Sample YAML definition file for nginx service at ClusterIP:
--------------------------------------------------------------
apiVersion: v1
kind: Service
metadata:
  name: nginx-service
spec:
  selector:
    app: nginx
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
  type: ClusterIP

Samnple NGINX Deployment YAML :
---------------------------------

apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 2
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        ports:
        - containerPort: 80


2. NodePort Service :
------------------------
-> Expose resources externally (it has a range 30000-32767)
-> NodePort exposes the service on each Node’s IP at a static port (the NodePort)
-> You can contact the NodePort service from outside the cluster, by requesting <Node IP>:<Node Port>

-> It allows users to access a service from outside the cluster using the node’s IP address and the assigned port.
-> Kubernetes allocates a port from a predefined range (default: 30000–32767).
-> This port is open on every node in the cluster.
-> Traffic sent to this port on any node is forwarded to the corresponding service and then to the appropriate Pod.

Sample YAML definition for NOdePort service :
---------------------------------------------------

apiVersion: v1
kind: Service
metadata:
  name: nginx-nodeport-service
spec:
  selector:
    app: nginx
  type: NodePort
  ports:
    - port: 80
      targetPort: 80
      nodePort: 30080

When to Use NodePort:
-----------------------------
-> For development or testing environments.
-> When you don’t have a cloud load balancer.
-> When you want direct access to services from outside the cluster.

3. Load Balancer Service : A LoadBalancer service in Kubernetes is a type of service that exposes your application to the internet by provisioning an external load balancer 
-----------------------------
(typically provided by a cloud provider like AWS, Azure, or GCP).

-> Expose resources externally
-> This service will use or dynamically create an external load balancer like a cloud load balancer when running in the cloud. This load balancer gets a public IP address.
-> It forwards external traffic to the Kubernetes service, which then routes it to the appropriate Pods.
-> Cloud provider will provide a mechanism for routing the traffic to the services
-> we can create network load balancer in cloud . cost will be there.

Sample YAML definition file for nginx load balancer:
----------------------------------------------------------
apiVersion: v1
kind: Service
metadata:
  name: nginx-loadbalancer-service
spec:
  selector:
    app: nginx
  type: LoadBalancer
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80

External Name Service:
------------------------
An ExternalName service in Kubernetes is a special type of service that maps a service name to an external DNS name. It doesn’t create a traditional service proxy or load balancer—instead, it acts like a DNS alias within the cluster.

Sample YAML definition file for external Name 
---------------------------------------------------
apiVersion: v1
kind: Service
metadata:
  name: external-db
spec:
  type: ExternalName
  externalName: db.example.com


Ingress : is a service which supports path based routing 
**********

Ingress in Kubernetes is an API object that manages external access to services within a cluster, typically over HTTP and HTTPS. It acts as a smart router that controls how requests from outside the cluster are routed to internal services based on rules like hostnames and URL paths.

=> Why Do We Need Ingress?
*********************************
By default, Kubernetes services are only accessible:

    -> Internally via ClusterIP
    -> Externally via NodePort or LoadBalancer, which are limited and not flexible

Ingress solves this by:

    -> Consolidating access under a single IP or domain
    -> Allowing path-based or host-based routing
    -> Supporting TLS termination (HTTPS)
    -> Enabling custom routing rules and authentication

How path based routing works ?
*************************************
1. Ingress Controller 
    -> Listens for incoming HTTP/HTTPS traffic from outside the cluster
    -> responsible for routing external HTTP(S) traffic to the appropriate services inside the Kubernetes cluster.

Popular Ingress Controllers :
********************************
NGINX - Most widely used, flexible and well-documented.
Traefik - Dynamic configuration, good for microservices.
HAProxy - High-performance, enterprise-grade.
Istio Gateway - Part of the Istio service mesh.
AWS ALB Ingress - Integrates with AWS Application Load Balancer.

2. Ingress resource 
    -> Define routing rules based on URL paths (e.g. /api/, /web, /admin)

3. Path rules
    -> Requests to /api are routed to Backend Service A.
    -> Requests to /web go to Backend Service B.
    -> Requests to /admin are sent to Backend Service C.

4. Backend Services
    -> Each service forwards traffic to its associated Pods (e.g., Pod A1, Pod A2).

Benefits of path based routing:
*************************************
-> Efficiently route different parts of an application (e.g. frontend, backend, admin) through a single domain
-> Simplifies traffic management and scaling.
-> Enables microservices architecture with clean separation.

Here's a sample Ingress YAML configuration for path-based routing in Kubernetes:

apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: example-ingress
  namespace: default
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - host: myapp.example.com
    http:
      paths:
      - path: /api
        pathType: Prefix
        backend:
          service:
            name: backend-service-a
            port:
              number: 80
      - path: /web
        pathType: Prefix
        backend:
          service:
            name: backend-service-b
            port:
              number: 80
      - path: /admin
        pathType: Prefix
        backend:
          service:
            name: backend-service-c
            port:
              number: 80

Explanation :
*****************
host: The domain name (e.g., myapp.example.com) that routes traffic through this Ingress.
paths: Each path (like /api, /web, /admin) is routed to a different backend service.
rewrite-target: Ensures the path is rewritten correctly when passed to the backend (optional, depends on your use case and ingress controller).



Kubernetes Deployment :
******************************
-> A deployment is an object in Kubernetes that lets you manage a set of identical pods
-> Ensure that a certain no. of replicas of pods are always running
-> Easily autoscale your applications using a Kubernetes deployment  (kubectl scale deployment <deployment-name> --replicas=<number-of-replicas> -n <ns>)
-> Rolling update & Roll back we can do in case of Deployment
-> Stateless applications are deployed using Deployment in K8S

-> Creates and manages pods automatically
-> Scale the number of replicas up or down
-> does rolling update and rollback

How it works ?
-------------------
1. You define a Deployment in a YAML file (or via kubectl).
2. Kubernetes creates a ReplicaSet based on the Deployment spec.
3. The ReplicaSet ensures the desired number of Pods are running.
4. If you update the Deployment(rolling update) (e.g., change the image), then Kubernetes:
    -> Creates a new ReplicaSet.
    -> Gradually shifts traffic from the old Pods to the new ones (rolling update).
    -> Deletes the old Pods once the new ones are ready.

Deployment Rolling update and Rollback commands :
*********************************************************
-> A rolling update allows you to update the Deployment (e.g., change the container image) without downtime.

# kubectl set image deployment/nginx-deployment nginx=nginx:1.21.0   (nginx image version is udpated to nginx:1.21.0)

-> This updates the container image in the Deployment to nginx:1.21.0.
-> Kubernetes will gradually replace old Pods with new ones.

Rollback to Previous Version :
------------------------------------

# kubectl rollout undo deployment/nginx-deployment

=> This reverts the Deployment to the last known good configuration.

Check rollout status :
--------------------------
# kubectl rollout status deployment/nginx-deployment

View Revision History :
------------------------------
# kubectl rollout history deployment/nginx-deployment



Sample deployment manifest yaml file :
-------------------------------------------
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        ports:
        - containerPort: 80


StatefulSet :
---------------
A StatefulSet is a Kubernetes workload API object used to manage stateful applications
Unlike Deployments, which are designed for stateless applications, StatefulSets are used when each Pod needs a unique identity, stable storage, and ordered deployment or scaling.

Key Features of StatefulSet:
--------------------------------
1. Stable Network Identity

    -> Each Pod gets a unique, stable DNS name (e.g., pod-0, pod-1, etc.).
    -> Useful for applications like databases that require fixed hostnames.

2. Stable Persistent Storage

    -> Each Pod can be associated with a PersistentVolumeClaim (PVC) that is not deleted when the Pod is rescheduled.
    -> Ensures data is preserved across restarts.

3. Ordered Deployment and Scaling

    -> Pods are created, updated, and deleted in order (e.g., pod-0 before pod-1).
    -> Ensures predictable behavior for clustered applications.

4. Graceful Rolling Updates
    -> Updates are done one Pod at a time, maintaining order and stability.

How It Works? :
----------------
When you create a StatefulSet with 3 replicas:
    -> Kubernetes creates Pods named myapp-0, myapp-1, and myapp-2.
    -> Each Pod gets its own PVC (e.g., myapp-pvc-0, myapp-pvc-1, etc.).
    -> If a Pod is deleted, it is recreated with the same name and volume.

Deployment and Statefulset difference :
*********************************************

1. Pod identity : in case of deployment pod identity is anonymous(random string at the end of each pod name), but in case of statefulset, pod identity would be unique or stable like(pod-0, pod-1, pod-2)

2. Storage : in case of deployment storage(pvc) is shared or ephemeral, but in case of statefulset storage(pvc) is dedicated for each pod (persistent per pod)

3. Use case - stateless application((e.g., web servers, rest api, microservices)) are deployed using deployment and statefulset applications(e.g., databases(mongodb, timescaledb, mysql), Kafka, rabbitmq) are deployed using statefulset

4. Pod management - in case of deployment it is parallel but in case of statefulset it is ordered (sequential)

5. DNS - shared service name in case of deployment but in case of statefulset it is Unique DNS per Pod.

Sample manifest yaml file for TimescaleDB statefulset :
---------------------------------------------------------------

apiVersion: v1
kind: Service
metadata:
  name: timescaledb-headless
  labels:
    app: timescaledb
spec:
  ports:
    - port: 5432
  clusterIP: None
  selector:
    app: timescaledb
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: timescaledb
spec:
  selector:
    matchLabels:
      app: timescaledb
  serviceName: "timescaledb-headless"
  replicas: 1
  template:
    metadata:
      labels:
        app: timescaledb
    spec:
      containers:
      - name: timescaledb
        image: timescale/timescaledb:latest-pg12
        ports:
        - containerPort: 5432
          name: postgres
        env:
        - name: POSTGRES_PASSWORD
          value: "yourpassword"
        volumeMounts:
        - name: timescale-storage
          mountPath: /var/lib/postgresql/data
  volumeClaimTemplates:
  - metadata:
      name: timescale-storage
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: 1Gi


What is a DaemonSet?
***************************
-> A DaemonSet ensures that a copy of a Pod runs on all (or some) nodes in the cluster. It’s typically used for background system-level tasks.

Common Use Cases:
---------------------
-> Log collection agents (e.g., Fluentd, Logstash)
-> Monitoring agents (e.g., Prometheus Node Exporter)
-> Network plugins (e.g., Calico, Cilium)
-> Storage daemons (e.g., GlusterFS, Ceph)

Sample YAML definition file for Daemonset:

apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: log-agent
spec:
  selector:
    matchLabels:
      name: log-agent
  template:
    metadata:
      labels:
        name: log-agent
    spec:
      containers:
      - name: log-agent
        image: fluentd


Node/Pod affinity and anti-affinity :
************************************
In Kubernetes, affinity and anti-affinity rules help control where pods are scheduled in the cluster based on node or pod characteristics. These rules are useful for improving performance, availability, and fault tolerance.

Node Affinity : Node affinity is like a filter that tells Kubernetes which nodes a pod can be scheduled on, based on node labels.

Example Use Cases:
--------------------
-> Schedule pods only on nodes with SSDs.
-> Avoid nodes in a specific zone.

Types:
-> requiredDuringSchedulingIgnoredDuringExecution: Hard rule — pod won’t be scheduled unless the rule is met.
-> preferredDuringSchedulingIgnoredDuringExecution: Soft rule — scheduler tries to meet it but can ignore if needed.

Example of nodeAffinity (Schedule pods only on nodes with SSDs.):

affinity:
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
        - matchExpressions:
            - key: disktype
              operator: In
              values:
                - ssd

2. Pod Affinity :
-> Pod affinity tells Kubernetes to schedule a pod near other pods (on the same node or in the same topology domain like zone or region).

Example Use Cases:
---------------------
-> Co-locate microservices that communicate frequently.
-> Improve performance by reducing latency.

 Example:
---------------
affinity:
  podAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchLabels:
            app: frontend
        topologyKey: "kubernetes.io/hostname"


 3. Pod Anti-Affinity :
---------------------------
-> Pod anti-affinity tells Kubernetes to avoid placing a pod near certain other pods.

Example Use Cases:
-----------------------
-> Spread replicas of the same app across nodes for high availability.
-> Avoid resource contention.


affinity:
  podAntiAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchLabels:
            app: my-app
        topologyKey: "kubernetes.io/hostname"



ConfigMap in k8s :
---------------------
-> In Kubernetes (K8s), a ConfigMap is an API object used to store non-confidential configuration data in key-value pairs.

Why Use ConfigMaps?
-----------------------
-> To externalize configuration from your application code.
-> To reuse the same container image in different environments (e.g., dev, staging, prod) with different configurations.
-> To update configuration without rebuilding your container image.

-> Environment Variables: ConfigMap values are injected into the container as environment variables.
-> Volume Mount: ConfigMap values are mounted as files inside the container (e.g., /etc/config).

Key Features
------------------
-> Stores data as key-value pairs.
-> Can be consumed by Pods as:
-> Environment variables
-> Command-line arguments
-> Configuration files in a volume

Sample YAMl file for configmap
-----------------------------------
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-config
data:
  APP_MODE: "production"
  LOG_LEVEL: "info"


Using ConfigMap in a Pod
-------------------------------

apiVersion: v1
kind: Pod
metadata:
  name: my-app
spec:
  containers:
  - name: app-container
    image: my-app-image
    env:
    - name: APP_MODE
      valueFrom:
        configMapKeyRef:
          name: my-config
          key: APP_MODE


Secrets :
************

In Kubernetes, a Secret is an object used to store sensitive data, such as passwords, OAuth tokens, SSH keys, or TLS certificates. Unlike ConfigMaps, Secrets are base64-encoded and can be configured to be more securely handled by the system.

Why Use Secrets?
-------------------
-> To protect sensitive information from being exposed in plain text.
-> To separate sensitive data from application code and configuration.
-> To control access to sensitive data using Kubernetes RBAC.

Sample YAMl file for secrets :
----------------------------------

apiVersion: v1
kind: Secret
metadata:
  name: my-secret
type: Opaque
data:
  username: YWRtaW4=        # base64 for "admin"
  password: c2VjcmV0MTIz    # base64 for "secret123"


###########

You can encode values using:
# echo -n 'admin' | base64

Using Secrets in a Pod:
-----------------------------

apiVersion: v1
kind: Pod
metadata:
  name: secret-env-pod
spec:
  containers:
  - name: my-container
    image: my-app-image
    env:
    - name: USERNAME
      valueFrom:
        secretKeyRef:
          name: my-secret
          key: username
    - name: PASSWORD
      valueFrom:
        secretKeyRef:
          name: my-secret
          key: password

As Mounted Files:
**********************

apiVersion: v1
kind: Pod
metadata:
  name: secret-volume-pod
spec:
  containers:
  - name: my-container
    image: my-app-image
    volumeMounts:
    - name: secret-volume
      mountPath: "/etc/secret-data"
      readOnly: true
  volumes:
  - name: secret-volume
    secret:
      secretName: my-secret

This will mount the secret keys as files under /etc/secret-data/username and /etc/secret-data/password.

How it works ?
--------------------
-> Secret: Stores sensitive data like username and password.
-> Pod: The application container that consumes the secret.
-> Environment Variable: The secret is injected into the container as environment variables.
-> Volume Mount: The secret is mounted as files inside the container (e.g., /etc/secret-data/username).


Kubernetest Service Account :
***********************************

=> A ServiceAccount in kubernetes is an identity for processes running in pods. It allows those processes to authenticate to the kubernetes API server and perform actions based on their assigned permissions.

=> Think of it as a "user account" for your application inside the cluster.

Default Behavior :
--------------------
-> Every namespace has a default service account
-> If you don't specify one, k8s automatically assigns the default ServiceAccount to your pod.
-> The ServiceAccount token is mounted into the Pod at :
    /var/run/secrets/kubernetes.io/serviceaccount/token

Creating a Custom ServiceAccount :
-------------------------------------
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-service-account
  namespace: default

Attach roles with RBAC:
*****************************

apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: default
  name: pod-reader
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "watch", "list"]

Create RoleBinding :
---------------------------

apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: read-pods
  namespace: default
subjects:
- kind: ServiceAccount
  name: my-service-account
  namespace: default
roleRef:
  kind: Role
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io

 Use ServiceAccount in a Pod :
*******************************

apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  serviceAccountName: my-service-account
  containers:
  - name: my-container
    image: busybox
    command: ["sleep", "3600"]


Use ServiceAcoount in a Deployment :
****************************************

apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app
spec:
  replicas: 2
  selector:
    matchLabels:
      app: my-app
  template:
    metadata:
      labels:
        app: my-app
    spec:
      serviceAccountName: my-service-account
      containers:
      - name: my-container
        image: busybox
        command: ["sh", "-c", "while true; do echo Hello from $(hostname); sleep 10; done"]


Persistent Volumes and Persistent Volumes Claim :
*****************************************************

Why PV & PVCs matter in k8s ?
-----------------------------------
=> Kubernetes pods are ephemeral—they can be created and destroyed at any time. If you store data inside a pod, it will be lost when the pod is deleted. To solve this, Kubernetes provides Persistent Volumes (PVs) and Persistent Volume Claims (PVCs).


=> In Kubernetes, Persistent Volumes(PV) and Persistent Volumes Claim(PVC) are used to manage persistent storage for pods. They decouple storage provisioning from pod lifecycle allowing data to persist even if pods are deleted or rescheduled

What is Persistent Volume(PV):
********************************
-> A persistent volume is a piece of storage in the cluster that has been provisioned by an administrator or dynamically provisioned through storage class. It’s like a physical disk in the cluster.
-> PersistentVolumes are a cluster-level resource like nodes, which don’t belong to any namespace
-> It has 3 things, one is Capacity(e.g. 5Gi), 2nd is accessModes(ReadOnlyMany, ReadWriteOnce, ReadWriteMany) and 3rd is Reclaim Policy (Retain, Recycle, Delete)

=> PVC : In Kubernetes (K8s), a Persistent Volume Claim (PVC) is a way for a user or application to request persistent storage from the cluster.

Key Concepts
----------------
Persistent Volume (PV): A piece of storage in the cluster, provisioned by an admin or dynamically via a StorageClass.
Persistent Volume Claim (PVC): A request for storage by a user. It specifies:
    -> Size (e.g., 10Gi)
    -> Access mode (e.g., ReadWriteOnce)
    -> Storage class (optional)


Key features :
-----------------
-> Cluster-Wide resource
-> Can be backed by local disk, NFS, cloud-storage(EBS, GCE, Azure-disk) etc
-> Lifecycle is independent of any individual pod
-> 3 main Yaml file attributes for PV:
    1. Capacity :  indicate its storage size in the Capacity attribute (e.g. 5 Gi)
    2. Access Modes : 
        -> ReadOnlyMany : multiple pods will use that pvc to read the data
        -> ReadWriteOnce : one pod will use that storage to read and write data
        -> ReadWriteMany : multiple pods will use that pvc to read and write data both
    3. ReclaimPolicy :
        -> Retain : volume will retain even if pv get deleted
        -> Recycle : meaning the data can be restored later after getting scrubbed
        -> Delete : associated storage assets (such as AWS EBS, GCE PD,..) are deleted

Sample Example of Persistent volume  yaml file :
----------------------------------------------------

apiVersion: v1
kind: PersistentVolume
metadata:
  name: my-pv
spec:
  capacity:
    storage: 5Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: /mnt/data


** Let’s walk through how to create a PersistentVolume (PV) using cloud storage like AWS EBS or Azure Disk, and how a PersistentVolumeClaim (PVC) can bind to it.

=> There are 2 ways of volume provisioning, one way is using static volume provisioning and other way is using dynamic volume provisioning through storage class.

AWS EBS-backed PersistentVolume : (Pre-requisites : kubernetes cluster running on AWS, EBS volume already created (or use dynamic provisioning))

Static PV Example (AWS EBS):
---------------------------------
apiVersion: v1
kind: PersistentVolume
metadata:
  name: aws-ebs-pv
spec:
  capacity:
    storage: 5Gi
  accessModes:
    - ReadWriteOnce
  awsElasticBlockStore:
    volumeID: <your-ebs-volume-id>
    fsType: ext4

Azure Disk-backed PersistentVolume : (Pre-requisites : Kubernetes cluster running on Azure (AKS), Azure Disk created or use dynamic provisioning)
--------------------------------------

apiVersion: v1
kind: PersistentVolume
metadata:
  name: azure-disk-pv
spec:
  capacity:
    storage: 5Gi
  accessModes:
    - ReadWriteOnce
  azureDisk:
    diskName: <your-disk-name>
    diskURI: <your-disk-uri>
    cachingMode: ReadOnly
    fsType: ext4
    kind: Managed

How to create Persistent volume claim from PV:
--------------------------------------------------

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
  volumeName: aws-ebs-pv  # or azure-disk-pv

Note : The volumeName field binds the PVC to a specific PV.

Use PVC in a Pod :
---------------------

apiVersion: v1
kind: Pod
metadata:
  name: app-using-pvc
spec:
  containers:
  - name: app
    image: nginx
    volumeMounts:
    - mountPath: "/usr/share/nginx/html"
      name: storage
  volumes:
  - name: storage
    persistentVolumeClaim:
      claimName: my-pvc


Dynamic Provisioning :
---------------------------
=> With dynamic provisioning, Kubernetes uses a StorageClass to automatically provision a volume when a PVC is created.

Storage Class :
------------------
-> Provisioner : determines the volume plug-in used by the storageClass (e.g., AWS EBS, GCE PD, CSI drivers).
-> Several plug-ins such as AWS EBS and GCE PD are available for different storage providers.
-> Parameters : contain available configurations accepted by the provisioner.
-> ReclaimPolicy : retain or delete
-> volumeBindingMode : Immediate/ WaitForFIrstConsumer
-> In PVC you have to mention that storage class name like (storageClassName: <name>)


Example Storage Class :
----------------------------

apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: fast-storage
provisioner: kubernetes.io/aws-ebs
parameters:
  type: gp2

Example: PVC Using StorageClass
---------------------------------------

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-dynamic-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
  storageClassName: fast-storage

When this PVC is created, Kubernetes:
----------------------------------------
-> Looks at the storageClassName.
-> Uses the provisioner defined in the StorageClass.
-> Dynamically creates a Persistent Volume.
-> Binds the new PV to the PVC.

Benefits of Dynamic Provisioning :
------------------------------------
-> No manual PV management.
-> Faster and more scalable.
-> Cloud-native: Works well with cloud providers and CSI drivers.


There are different types of storage class available for dynamic volume provisioning for on-premise k8s cluster as well as cloud deployed k8s cluster like(AKS, EKS, GKE)

In Kubernetes, StorageClasses define how dynamic volume provisioning should occur. The actual types of StorageClasses available depend on the cloud provider, CSI drivers, or on-premise storage solutions you're using.

Here are some of the most common on-premises dynamic volume provisioning options for Kubernetes:
---------------------------------------------------------------------------------------------------------
1. Rancher Local Path Provisioner
-> Provisioner: rancher.io/local-path
-> Use Case: Lightweight, simple dynamic provisioning using local disk paths
-> Best For: Development, testing, small-scale clusters.
-> Website: GitHub - rancher/local-path-provisioner

2. Longhorn
-> Provisioner: driver.longhorn.io
-> Use Case: Distributed block storage with replication, snapshots, and backups.
-> Best For: Production-grade on-prem clusters.
-> Integrated with Rancher.

3. NFS (via NFS-Client Provisioner)
-> Provisioner: nfs-client
-> Use Case: Dynamic provisioning using an existing NFS server.
-> Best For: Shared file storage across pods.

4. OpenEBS
-> Provisioners:
    -> openebs.io/local (Local PVs)
    -> openebs.io/jiva (Replicated block storage)
    -> cstor.csi.openebs.io (CStor engine)
-> Use Case: Container-attached storage with multiple engines.
-> Best For: Stateful workloads on bare metal.

5. NetApp Trident
-> Provisioner: Supports NetApp ONTAP (cloud or on-prem)
-> Use Case: Enterprise storage integration with Kubernetes.
-> Best For: Organizations using NetApp storage systems 1.


How to setup using Rancher Local Path Provisioner :
---------------------------------------------------------

Here’s a step-by-step guide to set up and use the Rancher Local Path Provisioner for dynamic volume provisioning in your on-prem Kubernetes cluster:

Step 1: Install Local Path Provisioner
# kubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/master/deploy/local-path-storage.yaml

This will:

-> Deploy the provisioner as a pod.
-> Create a StorageClass named local-path.

Step 2: Verify the StorageClass
-----------------------------------
# kubectl get storageclass

Step 3: Create a PVC Using Local Path
------------------------------------------
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: local-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 2Gi
  storageClassName: local-path

Apply it :
--------------
# kubectl apply -f pvc.yaml

Step 4: Use the PVC in a Pod
--------------------------------

apiVersion: v1
kind: Pod
metadata:
  name: test-pod
spec:
  containers:
    - name: test-container
      image: busybox
      command: [ "sleep", "3600" ]
      volumeMounts:
        - mountPath: "/data"
          name: local-storage
  volumes:
    - name: local-storage
      persistentVolumeClaim:
        claimName: local-pvc

Step 5: Check Where Data Is Stored :
----------------------------------------
By default, data is stored under /opt/local-path-provisioner on the node. You can customize this path in the provisioner’s config if needed.


What storage class is used for GKE cluster in production?
---------------------------------------------------------------
For production workloads on GKE (Google Kubernetes Engine), the recommended StorageClasses for dynamic volume provisioning depend on your performance and availability needs. Here's a breakdown of the most commonly used options.

1. standard (Balanced Persistent Disk)
-> Provisioner: pd.csi.storage.gke.io
-> Disk Type: Balanced Persistent Disk (SSD-backed)
-> Use Case: General-purpose workloads, web apps, databases.
-> Default: Often the default StorageClass in GKE clusters.

Here’s a sample PersistentVolumeClaim (PVC) YAML that uses the GKE CSI driver provisioner: pd.csi.storage.gke.io. This is the recommended provisioner for Google Persistent Disks in GKE.

 pvc-gke-csi.yaml
-----------------------

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: gke-csi-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 20Gi
  storageClassName: standard

Note : The standard StorageClass in GKE uses the pd.csi.storage.gke.io provisioner by default.

# kubectl get storageclass standard -o yaml

Look for this line in the output: provisioner: pd.csi.storage.gke.io

Here’s a complete example of a Kubernetes Deployment that uses a PersistentVolumeClaim (PVC) backed by the GKE CSI provisioner (pd.csi.storage.gke.io) via the standard StorageClass.

nginx-deployment-with-pvc.yaml
-----------------------------------

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: gke-csi-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 20Gi
  storageClassName: standard
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
        - name: nginx
          image: nginx:latest
          ports:
            - containerPort: 80
          volumeMounts:
            - name: nginx-storage
              mountPath: /usr/share/nginx/html
      volumes:
        - name: nginx-storage
          persistentVolumeClaim:
            claimName: gke-csi-pvc

What This Does:
------------------
-> Creates a PVC named gke-csi-pvc using the standard StorageClass.
-> Deploys an NGINX pod that mounts the PVC at /usr/share/nginx/html.
-> Any data written to that path will persist even if the pod is deleted or restarted.




Kubectl commands :
---------------------------
=> Deploying and Accessing an Application :

Let’s launch a simple Nginx web server and access its default web page, Execute below command to pull the Nginx image from Docker Hub and create a deployment called myweb:

# kubectl run --image=nginx:latest myweb  

It packages and deploys the container in a Kubernetes-specific artifact called a Pod. 
To access the web server running inside the Pod, (here target pod is port of pod)

# kubectl expose pod <pod_name> --type=NodePort --port=<expose_host_port> --target-port=<container_port>

example : # kubectl expose pod myweb-59d7488cb9-jvnwn --port=80 --target-port=80 --type=NodePort

The Pod is now exposed on every Node of the cluster on an arbitrary port.

(For a NodePort service, Kubernetes allocates a port from a configured range (default is 30000-32767), and each node forwards that port, which is the same on each node, to the service. It is possible to define a specific port number, but you should take care to avoid potential port conflicts.)

Note: A Service can map any incoming port to a targetPort. By default and for convenience, the targetPort is set to the same value as the port field.

# kubectl expose deployment <deployment_name> --port=<port> --target-port=<container_port> --type=NodePort

Then you can describe above service and get the NodePort details

# kubectl get svc <pod name/deployment name>

Use Kubectl commands :
***************************
1. Login inside pod : # kubectl exec -it <pod_name> -- /bin/bash
2. Describe Pod : # kubectl describe pod <pod name>
3. Get deployment/replica sets : # kubectl get deployment   # kubectl get rs
4. Get pods and show labels attached to those pods  # kubectl get pods --show-labels
5. Restart deployment : # kubectl rollout restart deploy <deployment name>
6. Get deployment status : # kubectl roolout status deployment <deployment name>

=> Rollout and rollback related commands for deployment

7. Update deployment image : # kubectl set image deployment/nginx-deployment nginx=nginx:1.21.0   (nginx image version is udpated to nginx:1.21.0)
8. Edit deployment object : kubectl edit deployment/helloworld-deployment       
9. Get rollout history : # kubectl rollout history deployment/hellowworld-deployment
10. Rollback to previous version : # kubectl rollout undo deployment helloworld-deployment
11. Rollback to specific version : # kubectl rollout undo deployment helloworld-deployment --to-revision=N


Cluster Introspection commands :
***********************************
1. Get version information : # kubectl version
2. Get cluster info : # kubectl cluster-info
3. Get the configuration : # kubectl config view
4. output information about a node : # kubectl describe node <node name>

Debugging commands:
*************************

View pod logs :
-------------------
1. View pod logs : # kubectl logs <pod-name>
2. View specific container logs inside pod : # kubectl logs <pod-name> -c <container-name>
3. Follow logs in realtime : # kubectl logs -f <pod-name>
4. Logs from last 1 hour : # kubectl logs --since=1h <pod_name> -c <container-name>
5. Logs since specific time : # kubectl logs --since-time="2025-05-24T14:00:00Z" <pod-name>
6. Other flags, show last 100 lines of logs
    # kubectl logs <pod-name> -c <container-name> --tail=100
    # kubectl logs <pod-name> -c <container-name> --tail=100 -f  (follow logs like tail -f)

How to see deployment logs :
----------------------------------
=> In kubernetes, deployment manage pods, but they don't have logs themselves. To view logs for a deployment, you need to access the logs of the pod it manages.

# Get the all pods managed by deployment : # kubectl get pods -l app=<deployment-label>
Replace in above command <deployment-label> with the label used in your Deployment (e.g., app=my-app)

=> Check logs for all pods in the deployment :

# kubectl get pods -l app=my-app -o name | xargs -I {} kubectl logs {}




Ingress :
***************
It exposes services to outside world, it maps your url to the services, you configure access by creating a collection of rules that defines which inbound connection reach to which services.

Ingress is not actually a type of service, instead it sits in front of multiuple services and acts as smart router or entrypoint into your cluster.


Lifecycle of Pod :
*************************

Phases of pod : 
1. pending : Accepted  by kubernetes but container not yet created
2. Running : Pod bound to a node, all containers created and at least one container is running/starting/re-starting
3. Succeeded : Container exited with status 0
4. Failed : All conatiner exit and at least one exited with non-zero status
5. Unknown : state of pod can not be determined due to communication issues with its node


































