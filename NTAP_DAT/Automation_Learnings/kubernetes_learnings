
Below are the node details for forming 5 node cluster:
-------------------------------------------------------
VM's Name    		  IP Address
-------------       :     ------------


Below machine is getting used as test controller :
-------------------------------------------------



To set up local k8s cluster and to install SDE on local setup:
--------------------------------------------------------------
Scripts location in shared NFS location: copy these all files from shares NFS directory of Rahul Sreenivasa to your machine using command :
-------------------------------------------
/u/rsreeniv/kubernetes_install_1_21_14		
/u/rsreeniv/sde_install				
/u/rsreeniv/cbs_install				
/u/rsreeniv/link_ontap_2_sde			

This is the main directory : /u/rsreeniv/kubernetes_install_1_21_14  : where you need to go and perform installation of k8s cluster setup

This is how you can copy all files from RahulSrinivasa machine to your's first VM i.e. which you want to keep as master

# mkdir /root/k8s_install; cd /root/k8s_install      ( on master & all worker nodes copy these files this way)

# cp -r /u/rsreeniv/kubernetes_install_1_21_14 /root/k8s_install
# cp -r /u/rsreeniv/sde_install /root/k8s_install
# cp -r /u/rsreeniv/cbs_install /root/k8s_install
# cp -r /u/rsreeniv/link_ontap_2_sde /root/k8s_install


# cd /etc/cni/net.d (after k8s install this directory contains cni related configuration files) 
# ls /etc/cni/net.d/
10-flannel.conflist  10-weave.conflist    (only these 2 files should be here on all nodes of k8s cluster, if anything else then remove that)


# To check ubuntu version on machine : lsb_release -d
# To get the ip address of linux machine : ifconfig ens192 | grep -Eo 'inet (addr:)?([0-9]*\.){3}[0-9]*'     (ifconfig <network_interface_name>)
# To get public and local ip of host, use command : hostname -I
# To see the hostname or server name use this command : hostname or check in this file: cat /etc/hosts
# To check free memory usage, use command : free
# To get cpu usage, use top command : top -d 5 | head -n 15
# To get uptime of machine, use command : uptime -s
# To check if you are root user, use this command : # id -u     , if o/p of this command is greater than 0, then that meansnon-root user, else 0 means root user

Steps to install k8s :
-------------------------
# First run master-k8s.sh file on master node

#./master-k8s.sh

once this script executes successfully then pods status check in kube-system namespace, by default all pods would be in kube-system namespace, in kube-flannel you can see some other pods :

# Check all available namespaces in k8s cluster :
----------------------------------------------------
root@cvs-k8s-jaynath-07:~/k8s_install/kubernetes_install_1_21_14# kubectl get ns
NAME              STATUS   AGE
default           Active   4m53s
kube-flannel      Active   4m7s
kube-node-lease   Active   4m55s
kube-public       Active   4m55s
kube-system       Active   4m55s

root@cvs-k8s-jaynath-07:~/k8s_install/kubernetes_install_1_21_14# kubectl get pods -n kube-system
NAME                                         READY   STATUS                  RESTARTS   AGE
coredns-558bd4d5db-ljgms                     1/1     Running                 0          87s
coredns-558bd4d5db-tfn6s                     1/1     Running                 0          87s
etcd-cvs-k8s-jaynath-07                      1/1     Running                 0          93s
kube-apiserver-cvs-k8s-jaynath-07            1/1     Running                 0          93s
kube-controller-manager-cvs-k8s-jaynath-07   1/1     Running                 0          93s
kube-proxy-lztxm                             1/1     Running                 0          87s
kube-scheduler-cvs-k8s-jaynath-07            1/1     Running                 0          93s
weave-net-lgxkp                              0/2     Init:ImagePullBackOff   0          87s





# if weave-net pod goes into init:ImagePullBackOff state, then describe that pod using below command :

# kubectl describe pod weave-net-lgxkp -n kube-system

If you see the issue like (Error: ImagePullBackOff) below in pod describe command output then fix this by following steps below :

Events:
  Type     Reason     Age                  From               Message
  ----     ------     ----                 ----               -------
  Normal   Scheduled  2m31s                default-scheduler  Successfully assigned kube-system/weave-net-lgxkp to cvs-k8s-jaynath-07
  Normal   Pulling    56s (x4 over 2m29s)  kubelet            Pulling image "weaveworks/weave-kube:latest"
  Warning  Failed     56s (x4 over 2m29s)  kubelet            Failed to pull image "weaveworks/weave-kube:latest": rpc error: code = Unknown desc = Error response from daemon: toomanyrequests: You have reached your pull rate limit. You may increase the limit by authenticating and upgrading: https://www.docker.com/increase-rate-limit
  Warning  Failed     56s (x4 over 2m29s)  kubelet            Error: ErrImagePull
  Warning  Failed     42s (x6 over 2m28s)  kubelet            Error: ImagePullBackOff
  Normal   BackOff    28s (x7 over 2m28s)  kubelet            Back-off pulling image "weaveworks/weave-kube:latest"

steps to fix this issue :

Since weave-net and kube-proxy and kube-flannel are kubernets object of type daemonset, since these all are related to configuring networking in k8s.

# kubectl edit ds weave-net -n kube-system   (fire this command)
# change imagepullPolicy to ifNotPresent from Always in daemonset edit file, so replace Always with ifNotPresent text on all occurences in that file
# Then do docker login with command on master node : docker login -u <jaynath120>   (password is : JayPatel#...)
# and now pull that image locally on your master : docker pull weaveworks/weave-kube:2.8.1

# in weave-net daemonset, edit the image name version from latest to 2.8.1  (better is lower version 2.8.1)

# kubectl edit ds weave-net -n kube-system     (in this weave-net ds config yaml file, change image version from latest to 2.8.1 ), otherwise if you want to
# go with latest version of weave-net image then do : docker pull weaveworks/weave-kube:latest, if you pull latest image then don't need to modify in weave-net cds config yaml file
 
spec:
      containers:
      - command:
        - /home/weave/launch.sh
        env:
        - name: INIT_CONTAINER
          value: "true"
        - name: HOSTNAME
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: spec.nodeName
        image: weaveworks/weave-kube:2.8.1       # change here in weave-net config yaml file of daemonset

	image: weaveworks/weave-npc:2.8.1
	
inti containers section :
image: weaveworks/weave-kube:2.8.1

#In this file kubectl edit ds weave-net -n kube-system , make changes at 3 places as mentioned above, change latest to specific version 2.8.1


# pull these 2 image on master node from docker manually for fixing weave-netpod issue, also pull these 2 images on all worker nodes as well after worker-k8s.sh installation
# docker login - jaynath120               (fire 3 below commands on all worker nodes as well as master)
# docker pull weaveworks/weave-npc:2.8.1
# docker pull weaveworks/weave-kube:2.8.1

weaveworks/weave-npc                             2.8.1               
weaveworks/weave-kube                            2.8.1 

# after making above change fire this command again to see the weave-net pod status :

# # kubectl get pods -n kube-system

Automatically after making changes in daemonset yml file of weave-net,once you save that file after chnages, then weave-net pod will come up in 20-30 seconds
# make sure all pods including, coredns, weave-net are running fine.

# Run worker-k8s.sh file on each node of cluster

# Join all worker nodes to master using below command :

# kubeadm token create --print-join-command    (use this command to generate token)

kubeadm join 10.193.227.49:6443 --token pzacwn.z49ayvj1ta56q519 --discovery-token-ca-cert-hash sha256:b460aba74c656e7f97cdb37434cb2fdb37ddde37eade2bb4f77166dd5b313302

Once token is generated, copy this kubeadm command and execute on all worker nodes

#fire this command to check all nodes joined :

# kubectl get nodes
NAME                 STATUS     ROLES                  AGE   VERSION
cvs-k8s-jaynath-07   Ready      control-plane,master   58m   v1.21.14
cvs-k8s-jaynath-08   Ready      <none>                 40s   v1.21.14
cvs-k8s-jaynath-09   Ready      <none>                 28s   v1.21.14
cvs-k8s-jaynath-10   NotReady   <none>                 13s   v1.21.14
cvs-k8s-jaynath-11   NotReady   <none>                 4s    v1.21.14


# kubectl get all --all-namespaces orkubectl get all -A   (to get all resources in all namespace)  check everything is up and ready till here 

NAMESPACE      NAME                                             READY   STATUS    RESTARTS   AGE
kube-flannel   pod/kube-flannel-ds-57xvm                        1/1     Running   0          59m
kube-flannel   pod/kube-flannel-ds-kpz47                        1/1     Running   0          88s
kube-flannel   pod/kube-flannel-ds-rwx9z                        1/1     Running   0          79s
kube-flannel   pod/kube-flannel-ds-sx5bs                        1/1     Running   0          115s
kube-flannel   pod/kube-flannel-ds-tv4c9                        1/1     Running   0          103s
kube-system    pod/coredns-558bd4d5db-ljgms                     1/1     Running   0          59m
kube-system    pod/coredns-558bd4d5db-tfn6s                     1/1     Running   0          59m
kube-system    pod/etcd-cvs-k8s-jaynath-07                      1/1     Running   0          59m
kube-system    pod/kube-apiserver-cvs-k8s-jaynath-07            1/1     Running   0          59m
kube-system    pod/kube-controller-manager-cvs-k8s-jaynath-07   1/1     Running   0          59m
kube-system    pod/kube-proxy-48v74                             1/1     Running   0          115s
kube-system    pod/kube-proxy-hv62t                             1/1     Running   0          88s
kube-system    pod/kube-proxy-lztxm                             1/1     Running   0          59m
kube-system    pod/kube-proxy-mgmhj                             1/1     Running   0          79s
kube-system    pod/kube-proxy-x9g8q                             1/1     Running   0          103s
kube-system    pod/kube-scheduler-cvs-k8s-jaynath-07            1/1     Running   0          59m
kube-system    pod/weave-net-2pbhd                              2/2     Running   1          88s
kube-system    pod/weave-net-d7lcb                              2/2     Running   2          115s
kube-system    pod/weave-net-f2mhc                              2/2     Running   1          17m
kube-system    pod/weave-net-fw698                              2/2     Running   1          79s
kube-system    pod/weave-net-zvvgq                              2/2     Running   1          103s

NAMESPACE     NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                  AGE
default       service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP                  60m
kube-system   service/kube-dns     ClusterIP   10.96.0.10   <none>        53/UDP,53/TCP,9153/TCP   60m

NAMESPACE      NAME                             DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
kube-flannel   daemonset.apps/kube-flannel-ds   5         5         5       5            5           <none>                   59m
kube-system    daemonset.apps/kube-proxy        5         5         5       5            5           kubernetes.io/os=linux   60m
kube-system    daemonset.apps/weave-net         5         5         5       5            5           <none>                   60m

NAMESPACE     NAME                      READY   UP-TO-DATE   AVAILABLE   AGE
kube-system   deployment.apps/coredns   2/2     2            2           60m

NAMESPACE     NAME                                 DESIRED   CURRENT   READY   AGE
kube-system   replicaset.apps/coredns-558bd4d5db   2         2         2       59m



## Important : at this point of time there won't be any storage class, persistent volume or persistent volume claim resources in k8s cluster, as you can check below :

root@cvs-k8s-jaynath-07:~/k8s_install/kubernetes_install_1_21_14# kubectl get pv -A
No resources found
root@cvs-k8s-jaynath-07:~/k8s_install/kubernetes_install_1_21_14# kubectl get pvc -A
No resources found in default namespace.
root@cvs-k8s-jaynath-07:~/k8s_install/kubernetes_install_1_21_14# kubectl get sc -A    (A- for all namespaces)





***********************************************************************************************************************************************

Steps 2 : Next step is to install helm3 and setting up persistent volume creation (for this you need to setup storage class)

How to install helm3, follow below commands and instructions :
---------------------------------------------------------------

#Get helm3 from this location and downloadit using curlas below command mentioned :

curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3

# above command will download helm3 executable file from that url and save it as output in get_helm.sh
# after that change executable permission of get_helm3.sh file as 777
# chmod 700 get_helm.sh	
# Execute that get_helm.sh file to install helm3 as (./get_helm.sh)	
# sudo ./get_helm.sh


# check helm version after installing helm :

# helm version
version.BuildInfo{Version:"v3.10.3", GitCommit:"835b7334cfe2e5e27870ab3ed4135f136eecc704", GitTreeState:"clean", GoVersion:"go1.18.9"}

root@cvs-k8s-jaynath-07:~/k8s_install# helm ls
NAME    NAMESPACE       REVISION        UPDATED STATUS  CHART   APP VERSION
root@cvs-k8s-jaynath-07:~/k8s_install# helm repo ls
Error: no repositories to show


# Setup/Init helm, Add stable and testing repo of helm

# helm repo add stable https://charts.helm.sh/stable
# helm repo add testing https://helmcharts.qstack.com/testing
# helm repo update

# helm ls   (list helm repos/deployments)

# helm version  (list the version of helm installed)

# list allhelm repository added locally 

root@cvs-k8s-jaynath-07:~/k8s_install# helm repo ls
NAME    URL
stable  https://charts.helm.sh/stable
testing https://helmcharts.qstack.com/testing

root@cvs-k8s-jaynath-07:~/k8s_install# helm ls
NAME    NAMESPACE       REVISION        UPDATED STATUS  CHART   APP VERSION

# initially there won't be any helm deployment , so it is showing as empty.



# Now setup persistent volumes, you need to create storage class where you can create PV and PVC whill will be allocated to containers for volume read/write of data. Before deployment of any pod using helm install/upgrade command, this step is mandatory otherwise containers within pod won't be in started state. they will go in pending state because volume would be in pending state
*************************************************************************************************************************

Initially by default there won't be any storage class, or pv or pvc existing in your cluster. You need to configure it manually. As you can see below , there is no sc, pv or pc initially.

root@cvs-k8s-jaynath-07:~/k8s_install# kubectl get sc
No resources found
root@cvs-k8s-jaynath-07:~/k8s_install# kubectl get pv
No resources found
root@cvs-k8s-jaynath-07:~/k8s_install# kubectl get pvc
No resources found in default namespace.

# Now let's create storage class using following steps, remember storage class is a global entity/objects in k8s, though you can create multiple storage class in k8s cluster, but in pv/pvc yaml file it need to be mntioned from which storage class it should create pv and pvc, so it will pickup from there, if it is not metnioned then it will pickup from default 
storage class.
When we create storage class we specify which storage class should be considered as default one
---------------------------------------------------------------------------------------------------------------------
Steps 1 : For creating storage class, you need to define  local-path-provisioner storage configuration.
	# Create default storage class on master node of k8s cluster
		# git clone https://github.com/rancher/local-path-provisioner.git		(you can clone this in any directory)
		
		
if you downloaded this above files from git, then go and check in local-path-provisioner/deploy/local-path-storage.yaml,
search for section (kind: StorageClass) as you can see below, we need to add annotations under metadata section, just below name section as you can see below :

# Add this line after name section of kind: StorageClass

annotations:
        storageclass.kubernetes.io/is-default-class: "true"

# After modifying , section would look like this as below :

# vim ./local-path-provisioner/deploy/local-path-storage.yaml

apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: local-path
  annotations:
        storageclass.kubernetes.io/is-default-class: "true"
provisioner: rancher.io/local-path
volumeBindingMode: WaitForFirstConsumer
reclaimPolicy: Delete

# after above changes, it would become default storage class

# Then apply that local-path-storage configuration using below command :
		# kubectl apply -f local-path-provisioner/deploy/local-path-storage.yaml


Output of above command : after executing above command, it will create local-path-storage namespace automatically
***************************
root@cvs-k8s-jaynath-01:~# kubectl apply -f local-path-provisioner/deploy/local-path-storage.yaml
namespace/local-path-storage created
serviceaccount/local-path-provisioner-service-account created
clusterrole.rbac.authorization.k8s.io/local-path-provisioner-role unchanged
clusterrolebinding.rbac.authorization.k8s.io/local-path-provisioner-bind unchanged
deployment.apps/local-path-provisioner created
storageclass.storage.k8s.io/local-path unchanged
configmap/local-path-config created


# Before applying that local-path-storage.yml file, namespace and all pods would be something like this :

root@cvs-k8s-jaynath-07:~/k8s_install# kubectl get ns
NAME              STATUS   AGE
default           Active   47h
kube-flannel      Active   47h
kube-node-lease   Active   47h
kube-public       Active   47h
kube-system       Active   47h
root@cvs-k8s-jaynath-07:~/k8s_install# kubectl get pods -A
NAMESPACE      NAME                                         READY   STATUS    RESTARTS   AGE
kube-flannel   kube-flannel-ds-57xvm                        1/1     Running   0          47h
kube-flannel   kube-flannel-ds-kpz47                        1/1     Running   0          46h
kube-flannel   kube-flannel-ds-rwx9z                        1/1     Running   0          46h
kube-flannel   kube-flannel-ds-sx5bs                        1/1     Running   0          46h
kube-flannel   kube-flannel-ds-tv4c9                        1/1     Running   0          46h
kube-system    coredns-558bd4d5db-ljgms                     1/1     Running   0          47h
kube-system    coredns-558bd4d5db-tfn6s                     1/1     Running   0          47h
kube-system    etcd-cvs-k8s-jaynath-07                      1/1     Running   0          47h
kube-system    kube-apiserver-cvs-k8s-jaynath-07            1/1     Running   0          47h
kube-system    kube-controller-manager-cvs-k8s-jaynath-07   1/1     Running   20         47h
kube-system    kube-proxy-48v74                             1/1     Running   0          46h
kube-system    kube-proxy-hv62t                             1/1     Running   0          46h
kube-system    kube-proxy-lztxm                             1/1     Running   0          47h
kube-system    kube-proxy-mgmhj                             1/1     Running   0          46h
kube-system    kube-proxy-x9g8q                             1/1     Running   0          46h
kube-system    kube-scheduler-cvs-k8s-jaynath-07            1/1     Running   19         47h
kube-system    weave-net-2pbhd                              2/2     Running   1          46h
kube-system    weave-net-d7lcb                              2/2     Running   2          46h
kube-system    weave-net-f2mhc                              2/2     Running   1          46h
kube-system    weave-net-fw698                              2/2     Running   1          46h
kube-system    weave-net-zvvgq                              2/2     Running   1          46h

# After applying local-path-storage.yml file, namespace local-path-storage, deployment.apps/local-path-provisioner, storageclass.storage.k8s.io/local-path , and configmap/local-path-config will be created.

root@cvs-k8s-jaynath-07:~/k8s_install# kubectl apply -f local-path-provisioner/deploy/local-path-storage.yaml
namespace/local-path-storage created
serviceaccount/local-path-provisioner-service-account created
clusterrole.rbac.authorization.k8s.io/local-path-provisioner-role created
clusterrolebinding.rbac.authorization.k8s.io/local-path-provisioner-bind created
deployment.apps/local-path-provisioner created
storageclass.storage.k8s.io/local-path created
configmap/local-path-config created



root@cvs-k8s-jaynath-07:~/k8s_install# kubectl get ns
NAME                 STATUS   AGE
default              Active   47h
kube-flannel         Active   47h
kube-node-lease      Active   47h
kube-public          Active   47h
kube-system          Active   47h
local-path-storage   Active   2m21s


root@cvs-k8s-jaynath-07:~/k8s_install# kubectl get pods -A
NAMESPACE            NAME                                         READY   STATUS    RESTARTS   AGE
kube-flannel         kube-flannel-ds-57xvm                        1/1     Running   0          47h
kube-flannel         kube-flannel-ds-kpz47                        1/1     Running   0          46h
kube-flannel         kube-flannel-ds-rwx9z                        1/1     Running   0          46h
kube-flannel         kube-flannel-ds-sx5bs                        1/1     Running   0          46h
kube-flannel         kube-flannel-ds-tv4c9                        1/1     Running   0          46h
kube-system          coredns-558bd4d5db-ljgms                     1/1     Running   0          47h
kube-system          coredns-558bd4d5db-tfn6s                     1/1     Running   0          47h
kube-system          etcd-cvs-k8s-jaynath-07                      1/1     Running   0          47h
kube-system          kube-apiserver-cvs-k8s-jaynath-07            1/1     Running   0          47h
kube-system          kube-controller-manager-cvs-k8s-jaynath-07   1/1     Running   20         47h
kube-system          kube-proxy-48v74                             1/1     Running   0          46h
kube-system          kube-proxy-hv62t                             1/1     Running   0          46h
kube-system          kube-proxy-lztxm                             1/1     Running   0          47h
kube-system          kube-proxy-mgmhj                             1/1     Running   0          46h
kube-system          kube-proxy-x9g8q                             1/1     Running   0          46h
kube-system          kube-scheduler-cvs-k8s-jaynath-07            1/1     Running   19         47h
kube-system          weave-net-2pbhd                              2/2     Running   1          46h
kube-system          weave-net-d7lcb                              2/2     Running   2          46h
kube-system          weave-net-f2mhc                              2/2     Running   1          46h
kube-system          weave-net-fw698                              2/2     Running   1          46h
kube-system          weave-net-zvvgq                              2/2     Running   1          46h
local-path-storage   local-path-provisioner-849cb58dff-88sfs      1/1     Running   0         


# after applying that configuration, in local-path-storage namespace, local-path-provisioner-xxxx pod would be created as you can see below :

root@cvs-k8s-jaynath-07:~/k8s_install# kubectl get pods -n local-path-storage

NAME                                      READY   STATUS    RESTARTS   AGE
local-path-provisioner-849cb58dff-88sfs   1/1     Running   0          4m2s


# Now you can see storage class created , and this is default one

root@cvs-k8s-jaynath-07:~/k8s_install# kubectl get sc
NAME                   PROVISIONER             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE
local-path (default)   rancher.io/local-path   Delete          WaitForFirstConsumer   false                  4m53s

root@cvs-k8s-jaynath-07:~/k8s_install# kubectl get pv
No resources found
root@cvs-k8s-jaynath-07:~/k8s_install# kubectl get pvc
No resources found in default namespace.

At this point there won't be any pv or pvc, only storage class would be created, 

Important : test creation of PV and PVC to ensure things are working before going for sde installation:
----------------------------------------
=> But if you want to test whether you are able to create pv or pvc in that storage class you can check using below commands, this is just for tetsing purpose, in real time by default pv and pvc would be created when containers would run


# kubectl apply -f local-path-provisioner/examples/pvc/pvc.yaml

# vim local-path-provisioner/examples/pvc/pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: local-path-pvc
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: local-path
  resources:
    requests:
      storage: 128Mi

As you can see above in yml, volumes of size 128 mb would be created and attached to pods/containers later in next step

Create PVC first :

# kubectl apply -f local-path-provisioner/examples/pvc/pvc.yaml

persistentvolumeclaim/local-path-pvc created

# Create a volume-test pod to check, pod is creating pvc internally

# kubectl apply -f local-path-provisioner/examples/pod/pod.yaml  

pod/volume-test created		(default name space)

# we are just created a pod with name volume-test in default namespace, just to check if pvc is getting attached to that pods or not which we have defined above.

# vim local-path-provisioner/examples/pod/pod.yaml

apiVersion: v1
kind: Pod
metadata:
  name: volume-test			# this is name of pod volume-test
spec:
  containers:
  - name: volume-test
    image: nginx:stable-alpine
    imagePullPolicy: IfNotPresent
    volumeMounts:
    - name: volv
      mountPath: /data
    ports:
    - containerPort: 80
  volumes:
  - name: volv
    persistentVolumeClaim:
      claimName: local-path-pvc		# This is the name of pvc local-path-pvc which created earlier and attaching to pod
	  


root@cvs-k8s-jaynath-07:~/k8s_install# kubectl get pvc
NAME             STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
local-path-pvc   Pending                                      local-path     31s
root@cvs-k8s-jaynath-07:~/k8s_install# kubectl get pvc
NAME             STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
local-path-pvc   Pending                                      local-path     35s
root@cvs-k8s-jaynath-07:~/k8s_install# kubectl describe pvc local-path-pvc
Name:          local-path-pvc
Namespace:     default
StorageClass:  local-path
Status:        Pending
Volume:
Labels:        <none>
Annotations:   volume.beta.kubernetes.io/storage-provisioner: rancher.io/local-path
               volume.kubernetes.io/selected-node: cvs-k8s-jaynath-10
Finalizers:    [kubernetes.io/pvc-protection]
Capacity:
Access Modes:
VolumeMode:    Filesystem
Used By:       volume-test
Events:
  Type    Reason                Age                From                                                                                                Message
  ----    ------                ----               ----                                                                                                -------
  Normal  WaitForPodScheduled   45s                persistentvolume-controller                                                                         waiting for pod volume-test to be scheduled
  Normal  Provisioning          45s                rancher.io/local-path_local-path-provisioner-849cb58dff-88sfs_a5b075a1-3c0b-4622-ada8-363cda573d3e  External provisioner is provisioning volume for claim "default/local-path-pvc"
  Normal  ExternalProvisioning  14s (x4 over 45s)  persistentvolume-controller                                                                         waiting for a volume to be created, either by external provisioner "rancher.io/local-path" or manually created by system administrator



root@cvs-k8s-jaynath-07:~/k8s_install# kubectl get pods -n local-path-storage
NAME                                                         READY   STATUS         RESTARTS   AGE
helper-pod-create-pvc-912d9bf6-7e09-44a3-b0b4-03fdd6b1171e   0/1     ErrImagePull   0          68s
local-path-provisioner-849cb58dff-88sfs                      1/1     Running        0          25m

root@cvs-k8s-jaynath-07:~/k8s_install# kubectl describe pod helper-pod-create-pvc-912d9bf6-7e09-44a3-b0b4-03fdd6b1171e -n local-path-storage

 Type     Reason          Age                From     Message
  ----     ------          ----               ----     -------
  Normal   SandboxChanged  87s                kubelet  Pod sandbox changed, it will be killed and re-created.
  Normal   Pulling         39s (x3 over 88s)  kubelet  Pulling image "busybox"
  Warning  Failed          39s (x3 over 88s)  kubelet  Failed to pull image "busybox": rpc error: code = Unknown desc = Error response from daemon: toomanyrequests: You have reached your pull rate limit. You may increase the limit by authenticating and upgrading: https://www.docker.com/increase-rate-limit
  Warning  Failed          39s (x3 over 88s)  kubelet  Error: ErrImagePull
  Normal   BackOff         14s (x6 over 84s)  kubelet  Back-off pulling image "busybox"
  Warning  Failed          14s (x6 over 84s)  kubelet  Error: ImagePullBackOff
  
  
  
 # For fixing above issue, pull busybox image on all nodes of cluster
	# docker login -u jaynath120
	# docker pull busybox



root@cvs-k8s-jaynath-07:~/k8s_install# kubectl get pods
NAME          READY   STATUS             RESTARTS   AGE
volume-test   0/1     ImagePullBackOff   0          11m

root@cvs-k8s-jaynath-07:~/k8s_install# kubectl describe pod volume-test

 Type     Reason            Age                    From               Message
  ----     ------            ----                   ----               -------
  Warning  FailedScheduling  11m                    default-scheduler  0/5 nodes are available: 5 persistentvolumeclaim "local-path-pvc" not found.
  Warning  FailedScheduling  11m                    default-scheduler  0/5 nodes are available: 5 persistentvolumeclaim "local-path-pvc" not found.
  Normal   Scheduled         4m13s                  default-scheduler  Successfully assigned default/volume-test to cvs-k8s-jaynath-10
  Warning  Failed            2m55s (x6 over 4m12s)  kubelet            Error: ImagePullBackOff
  Normal   Pulling           2m41s (x4 over 4m13s)  kubelet            Pulling image "nginx:stable-alpine"
  Warning  Failed            2m41s (x4 over 4m12s)  kubelet            Failed to pull image "nginx:stable-alpine": rpc error: code = Unknown desc = Error response from daemon: toomanyrequests: You have reached your pull rate limit. You may increase the limit by authenticating and upgrading: https://www.docker.com/increase-rate-limit
  Warning  Failed            2m41s (x4 over 4m12s)  kubelet            Error: ErrImagePull
  Normal   BackOff           2m30s (x7 over 4m12s)  kubelet            Back-off pulling image "nginx:stable-alpine"


# for fixing above issue pull docker image on all nodes
	# docker pull nginx:stable-alpine
	
	
After pulling images automatically pod will come up as you can see below :

root@cvs-k8s-jaynath-07:~/k8s_install# kubectl get pvc
NAME             STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
local-path-pvc   Bound    pvc-912d9bf6-7e09-44a3-b0b4-03fdd6b1171e   128Mi      RWO            local-path     9m36s

root@cvs-k8s-jaynath-07:~/k8s_install# kubectl get pods
NAME          READY   STATUS    RESTARTS   AGE
volume-test   1/1     Running   0          14m


Note : All persistent volumes state should be in bound state, if it goes into any other state like pending, then please fix it before going further.


root@cvs-k8s-jaynath-07:~/k8s_install# kubectl describe pvc local-path-pvc
Name:          local-path-pvc
Namespace:     default
StorageClass:  local-path
Status:        Bound
Volume:        pvc-912d9bf6-7e09-44a3-b0b4-03fdd6b1171e
Labels:        <none>
Annotations:   pv.kubernetes.io/bind-completed: yes
               pv.kubernetes.io/bound-by-controller: yes
               volume.beta.kubernetes.io/storage-provisioner: rancher.io/local-path
               volume.kubernetes.io/selected-node: cvs-k8s-jaynath-10
Finalizers:    [kubernetes.io/pvc-protection]
Capacity:      128Mi
Access Modes:  RWO
VolumeMode:    Filesystem
Used By:       volume-test
Events:



# the above steps creating local-path-pvc and volume-test pod is just for testing purpoise, mandatoruly not required to be performed , if you want you can skip this as well.but better create pvc and pod for tetsing purpose to ensure things are working correctlty before proceeding further.



The above steps like installing helm and creating pvc and pod, instea of doing manually using abobe steps, you can also do by running sh file # ./master1.sh file  (/root/k8s_install/sde_install/master1.sh)
***********************************************************************************************************************

***Important : Now next step is to install timescale db setup:

# apt list postgresql postgresql-*		
# sudo apt-get --purge remove postgresql postgresql-* -y


# Timescale db installation steps from:  https://confluence.ngage.netapp.com/pages/viewpage.action?pageId=300303655
*********************************************************************************************************************



Timescale version supported: only till 0.7.1

Delete older if needed:
----------------------------

                # helm delete timescaledb

                # kubectl delete secret timescaledb-certificate timescaledb-credentials timescaledb-pgbackrest

                # kubectl delete pvc storage-volume-timescaledb-0 storage-volume-timescaledb-1    storage-volume-timescaledb-2 wal-volume-timescaledb-0 wal-volume-timescaledb-1 wal-volume-timescaledb-2

                # kubectl delete svc timescaledb-config
				
Install timescale:
---------------------

Before installing timescaledb, make sure to install it in sde namespace, so create a namespace sde
# kubectl create ns sde   or you can also install timescaledb in default namespace
					
					# helm repo add timescaledb 'https://charts.timescale.com/'
					
                    # helm fetch timescaledb/timescaledb-single --version=0.7.1    (timescaledb-single-0.7.1.tgz)
					# tar -xvzf timescaledb-single-0.7.1.tgz		(extract that tar file timescaledb-single-0.7.1.tgz)

## helm fetch command willonly download that chart to your local machine from helm repository in cureent directory


                    # cd timescaledb-single

                    # chmod 0777 generate_kustomization.sh
					
                    # ./generate_kustomization.sh timescaledb    
					
# (It will install and generate secrets for timescaledb by default in default namespace )

if you want to install timescaledb in sde namespace then secrets also needs to be generated in same sde namespace, so to generate secrets in sde namespace, you need to make changes in generate_kustomization.sh file.
# search in this file for "kubectl apply" and append -n sde to that line where kubectl apply is found, generally 2 lines would be there which you need to replace with -n sde



                    # cd ..

                    # helm install timescaledb-single --name-template=timescaledb -n sde      (add -n sde , if namespace)

#If you want to install  timescaledb in sde namespace, use this command : 
# helm fetch timescaledb/timescaledb-single --version=0.7.1 -n sde
#helm repo remove timescale
"timescale" has been removed from your repositories


root@cvs-k8s-jaynath-07:~/k8s_install/timescaledb-single# ./generate_kustomization.sh timescaledb
Can't load /root/.rnd into RNG
139731731427776:error:2406F079:random number generator:RAND_load_file:Cannot open file:../crypto/rand/randfile.c:88:Filename=/root/.rnd
Generating a RSA private key
.............................++++
.....................++++
writing new private key to './kustomize/timescaledb/tls.key'
-----
Do you want to configure the backup of your database to S3 (compatible) storage? (y/n)
n

Generated a kustomization named timescaledb in directory ./kustomize/timescaledb.


WARNING: The generated certificate in this directory is self-signed and is only
         fit for development and demonstration purposes.
         The certificate should be replaced by a signed certificate, signed by
         a Certificate Authority (CA) that you trust.


You may now wish to (p)review the files that have been created and further edit
them before deployment.


To preview the deployment of the secrets:

    kubectl kustomize "./kustomize/timescaledb"

Or you may want to install the secrets directly? (y/n)
y
Installing secrets...
secret/timescaledb-certificate created
secret/timescaledb-credentials created
secret/timescaledb-pgbackrest created



root@cvs-k8s-jaynath-07:~/k8s_install# helm install timescaledb-single --name-template=timescaledb
NAME: timescaledb
LAST DEPLOYED: Sat Jan 14 19:29:20 2023
NAMESPACE: default
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
TimescaleDB can be accessed via port 5432 on the following DNS name from within your cluster:
timescaledb.default.svc.cluster.local

To get your password for superuser run:

    # superuser password
    PGPASSWORD_POSTGRES=$(kubectl get secret --namespace default timescaledb-credentials -o jsonpath="{.data.PATRONI_SUPERUSER_PASSWORD}" | base64 --decode)

    # admin password
    PGPASSWORD_ADMIN=$(kubectl get secret --namespace default timescaledb-credentials -o jsonpath="{.data.PATRONI_admin_PASSWORD}" | base64 --decode)

To connect to your database, chose one of these options:

1. Run a postgres pod and connect using the psql cli:
    # login as superuser
    kubectl run -i --tty --rm psql --image=postgres \
      --env "PGPASSWORD=$PGPASSWORD_POSTGRES" \
      --command -- psql -U postgres \
      -h timescaledb.default.svc.cluster.local postgres

    # login as admin
    kubectl run -i --tty --rm psql --image=postgres \
      --env "PGPASSWORD=$PGPASSWORD_ADMIN" \
      --command -- psql -U admin \
      -h timescaledb.default.svc.cluster.local postgres

2. Directly execute a psql session on the master node

   MASTERPOD="$(kubectl get pod -o name --namespace default -l release=timescaledb,role=master)"
   kubectl exec -i --tty --namespace default ${MASTERPOD} -- psql -U postgres



### # superuser password
    PGPASSWORD_POSTGRES=$(kubectl get secret --namespace default timescaledb-credentials -o jsonpath="{.data.PATRONI_SUPERUSER_PASSWORD}" | base64 --decode)
	
# To get timescaledb secrets
# kubectl get secret --namespace default timescaledb-credentials -o jsonpath="{.data.PATRONI_SUPERUSER_PASSWORD}" | base64 --decode


# kubectl get pods

if you see any pods in errimagepull or imagepullbackoff , then edit the statefiulset and change imagePullPolicy to IfNotPresent from Always on all occurences.

# also do docker login on all nodes and pull that image which is failed to pull in # kubectl describe pod <podname> command

# kubectl edit sts timescaledb 



root@cvs-k8s-jaynath-07:~/k8s_install# kubectl get svc
NAME                  TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
kubernetes            ClusterIP      10.96.0.1       <none>        443/TCP          2d1h
timescaledb           LoadBalancer   10.107.56.225   <pending>     5432:31816/TCP   13m
timescaledb-config    ClusterIP      None            <none>        8008/TCP         13m
timescaledb-replica   ClusterIP      10.103.61.82    <none>        5432/TCP         13m

# change timescaledb service type from loadbalancer to NodePort
# kubectl edit svc timescaledb

in above command change type: LoadBalancer to NodePort and save it 

root@cvs-k8s-jaynath-07:~/k8s_install# kubectl get svc
NAME                  TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
kubernetes            ClusterIP   10.96.0.1       <none>        443/TCP          2d1h
timescaledb           NodePort    10.107.56.225   <none>        5432:31816/TCP   22m
timescaledb-config    ClusterIP   None            <none>        8008/TCP         22m
timescaledb-replica   ClusterIP   10.103.61.82    <none>        5432/TCP         22m



root@cvs-k8s-jaynath-07:~/k8s_install# kubectl get pvc
NAME                           STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
local-path-pvc                 Bound    pvc-912d9bf6-7e09-44a3-b0b4-03fdd6b1171e   128Mi      RWO            local-path     86m
storage-volume-timescaledb-0   Bound    pvc-36f46354-4342-430a-87d8-6fbf0382a2cb   2Gi        RWO            local-path     13m
storage-volume-timescaledb-1   Bound    pvc-055644d9-acfd-4a14-92c3-b7b8f31c66e0   2Gi        RWO            local-path     11m
storage-volume-timescaledb-2   Bound    pvc-050bd70e-e69c-4b4d-887c-49c81613f4c9   2Gi        RWO            local-path     4m37s
wal-volume-timescaledb-0       Bound    pvc-1ce8b732-fb40-42f9-9b5f-715ee6827bf1   1Gi        RWO            local-path     13m
wal-volume-timescaledb-1       Bound    pvc-c9529e2e-c4ec-42fd-ba34-812896cfd0fd   1Gi        RWO            local-path     11m
wal-volume-timescaledb-2       Bound    pvc-9f2faf37-cd36-421f-9629-a33c28a28271   1Gi        RWO            local-path     4m37s


root@cvs-k8s-jaynath-07:~/k8s_install# kubectl get pods
NAME            READY   STATUS    RESTARTS   AGE
timescaledb-0   1/1     Running   0          4m32s
timescaledb-1   1/1     Running   0          5m40s
timescaledb-2   1/1     Running   0          5m10s
volume-test     1/1     Running   0          92m


root@cvs-k8s-jaynath-07:~/k8s_install# kubectl get secrets
NAME                                TYPE                                  DATA   AGE
default-token-jvrqr                 kubernetes.io/service-account-token   3      2d1h
sh.helm.release.v1.timescaledb.v1   helm.sh/release.v1                    1      17m
timescaledb-certificate             kubernetes.io/tls                     2      27m
timescaledb-credentials             Opaque                                3      27m
timescaledb-pgbackrest              Opaque                                0      27m
timescaledb-token-p5vdj             kubernetes.io/service-account-token   3      17m


root@cvs-k8s-jaynath-07:~/k8s_install# kubectl get secrets timescaledb-credentials -o yaml

apiVersion: v1
data:
  PATRONI_REPLICATION_PASSWORD: SXJuZ2JnV0xKTXYzcXdsWVZxVUpJOEVPcXBnenhoS1U=
  PATRONI_SUPERUSER_PASSWORD: SU5IQlFFbnA4Nm1aZlVINjJSNXlDcm5NVDY0elV5ZlA=
  PATRONI_admin_PASSWORD: NVRIS3liRE1Sa2FiSkxMVTFQRGpMZVliMXNwcW1nTGs=
kind: Secret
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |

  creationTimestamp: "2023-01-14T19:19:34Z"
  labels:
    app: timescaledb-timescaledb
    cluster-name: timescaledb
  name: timescaledb-credentials
  namespace: default
  resourceVersion: "281166"
  uid: 011f20f3-0e01-413e-8aee-37f0716ca098
type: Opaque




# get that PATRONI_SUPERUSER_PASSWORD  password, this would be in encoded format and decode that value using command :

# echo SU5IQlFFbnA4Nm1aZlVINjJSNXlDcm5NVDY0elV5ZlA= | base64 -d


# echo SU5IQlFFbnA4Nm1aZlVINjJSNXlDcm5NVDY0elV5ZlA= | base64 -d

INHBQEnp86mZfUH62R5yCrnMT64zUyfP		=> This is decoded password

or you can decode that password using command :
# kubectl get secret --namespace default timescaledb-credentials -o jsonpath="{.data.PATRONI_SUPERUSER_PASSWORD}" | base64 --decode


INHBQEnp86mZfUH62R5yCrnMT64zUyf


***************************************************************************************
root@cvs-k8s-jaynath-07:~/k8s_install# kubectl get all
NAME                READY   STATUS    RESTARTS   AGE
pod/timescaledb-0   1/1     Running   0          19m
pod/timescaledb-1   1/1     Running   0          20m
pod/timescaledb-2   1/1     Running   0          20m
pod/volume-test     1/1     Running   0          107m

NAME                          TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
service/kubernetes            ClusterIP   10.96.0.1       <none>        443/TCP          2d1h
service/timescaledb           NodePort    10.107.56.225   <none>        5432:31816/TCP   29m
service/timescaledb-config    ClusterIP   None            <none>        8008/TCP         29m
service/timescaledb-replica   ClusterIP   10.103.61.82    <none>        5432/TCP         29m

NAME                           READY   AGE
statefulset.apps/timescaledb   3/3     29m


INHBQEnp86mZfUH62R5yCrnMT64zUyfProot@cvs-k8s-jaynath-07:~/k8s_install# kubectl get cm
NAME                     DATA   AGE
kube-root-ca.crt         1      2d1h
timescaledb-patroni      1      32m
timescaledb-pgbackrest   1      32m
timescaledb-scripts      8      32m



root@cvs-k8s-jaynath-07:~/k8s_install# helm ls
NAME            NAMESPACE       REVISION        UPDATED                                 STATUS          CHART                      APP VERSION
timescaledb     default         1               2023-01-14 19:29:20.207952058 +0000 UTC deployed        timescaledb-single-0.7.1


# If you want to install in sde namespace then follow this :

root@cvs-k8s-jaynath-07:~/k8s_install/timescaledb-single# ./generate_kustomization.sh timescaledb

# above command will generate secrets automatically in default namespace like timescaledb-certificate, timescaledb-credentials, timescaledb-pgbackrest, timescaledb-token-p5vdj

# If you want all secrets to be generated in sde namespace, then edit ./generate_kustomization.sh, and change in this file like below :

 To install these secrets, execute:

    kubectl apply -k "${KUSTOMIZE_DIR}" -n sde   => add this namespace sde in all occurences in that file for kubectl 

__EOT__
}

install_secrets() {
    echo "Installing secrets..."
    kubectl apply -k "${KUSTOMIZE_DIR}" -n sde		=> Add this sde namespace
}

# You can also see that after executing command ./generate_kustomization.sh timescaledb, if will generate below files in that directory mentioned : timescaledb-single/kustomize/timescaledb

root@cvs-k8s-jaynath-07:~/k8s_install/timescaledb-single/kustomize/timescaledb# ls
credentials.conf  kustomization.yaml  pgbackrest.conf  tls.crt  tls.key

# and then helm install in sde namespace for timescaledb
~/k8s_install.#  helm install timescaledb-single --name-template=timescaledb

# The other way you can generate those secrets in sde namespace is using below mentioned method

# This below command will customize all files credentials.conf, kustomization.yml, pgbackrest.conf, tls.crt, tls.key into one single file named timescaledbMap.yaml (kubectl kustomize ./ > timescaledbMap.yaml)

# we are just merging all files into single yaml file using command, kubectl kustomize

root@cvs-k8s-jaynath-07:~/k8s_install/timescaledb-single/kustomize/timescaledb# kubectl kustomize ./ > timescaledbMap.yaml

root@cvs-k8s-jaynath-07:~/k8s_install/timescaledb-single/kustomize/timescaledb# kubectl kustomize ./ > timescaledbMap.yaml
root@cvs-k8s-jaynath-07:~/k8s_install/timescaledb-single/kustomize/timescaledb# ls
credentials.conf  kustomization.yaml  pgbackrest.conf  timescaledbMap.yaml  tls.crt  tls.key

# Then apply using below command to generate secrets in sde namespace

root@cvs-k8s-jaynath-07:~/k8s_install/timescaledb-single/kustomize/timescaledb# kubectl apply -f timescaledbMap.yaml -n sde

~/k8s_install# helm install timescaledb-single --name-template=timescaledb -n sde

**********************************************************************************************************************
***********************************************************************************************************************

# After doing all this proceed with helm installation command :

# This is the repo where helm charts are present like nfsaas-deployment, https://helmcharts.qstack.com/testing/
# nfsaas-deployment-2023.1.0-RC09.tgz

Helm chart name which we want to install is : nfsaas-deployment-<release_version_with_date>.tgz



--set-string pgbouncer-kubernetes.global.postgresHost="timescaledb.default.svc.cluster.local"
-- set-string pgbouncer-kubernetes.global.managedPostgresHost="timescaledb.default.svc.cluster.local"



# Troubleshooting issues and pod containers :


issue 1: if after all sde pods comes up, and after that when you check sde version deployed on your cluster using below curl requests and if it timedout, then may be some problem with your pods running in kube-system namespace, so see helath status of all pods there in kube-system namespace. and fix all those pods to make in healthy state.

# curl http://10.193.227.49:30957/v2/projects/725117389761/locations/us-east4/version
curl: (28) Failed to connect to 10.193.227.49 port 30957 after 21031 ms: Timed out


Make sure all pods are healthy in kube-system namespace, specially weave net, coredns, kube-proxy pods,resolve these issues and then your curl command will work. If not work then follow below steps again, restart coredns, weave-net and kube-proxy in kube-system namespace

# kubectl rollout restart deploy coredns -n kube-system
deployment.apps/coredns restarted

# kubectl rollout restart daemonset weave-net -n kube-system
daemonset.apps/weave-net restarted

# kubectl rollout restart daemonset kube-proxy -n kube-system
daemonset.apps/kube-proxy restarted

# Now restart cloud-volumes-proxy pod again by deleting that :

# kubectl delete pod cloud-volumes-proxy-5bd84f8867-fxkjm -n sde

# Now check curl command output , it should work
# curl http://10.193.227.49:30865/v2/projects/725117389761/locations/us-east4/version

{"apiVersion":"1.4.0","sdeVersion":"2023.1.0-RC09"}


Issue2: Fixing rabbitmq issues : (may be issue with busybox image, so pull on all nodes, # docker pull busybox)

=> if nfsaas-rabbitmq pods goes into pending state, then follow below steps to fix that
	1. Check the rabbitmq pvc status : kubectl get pvc -n sde | grep rabbitmq
	2. If PVC state is pending , then delete that pvc , kubectl delete pvc command
	3. Then check all pods in local-path-storage namespace, kubectl get pods -n local-path-storage

root@cvs-k8s-jaynath-07:~# kubectl get pods -n local-path-storage
NAME                                      READY   STATUS    RESTARTS   AGE
local-path-provisioner-849cb58dff-88sfs   1/1     Running   0          19h 
	
There would be helper pod in local-path-storage,some thing like this name helper-pod-create-pvc-0f0dcf23-ca74-4203-b657-c43bcd136b30

# describe that pod and see what errors coming there :

kubectl describe pod helper-pod-create-pvc-0f0dcf23-ca74-4203-b657-c43bcd136b30 -n local-path-storage

----     ------          ----                ----     -------
  Normal   SandboxChanged  100s                kubelet  Pod sandbox changed, it will be killed and re-created.
  Normal   BackOff         25s (x6 over 98s)   kubelet  Back-off pulling image "busybox"
  Warning  Failed          25s (x6 over 98s)   kubelet  Error: ImagePullBackOff
  Normal   Pulling         11s (x4 over 101s)  kubelet  Pulling image "busybox"
  Warning  Failed          11s (x4 over 101s)  kubelet  Failed to pull image "busybox": rpc error: code = Unknown desc = Error response from daemon: toomanyrequests: You have reached your pull rate limit. You may increase the limit by authenticating and upgrading: https://www.docker.com/increase-rate-limit
  Warning  Failed          11s (x4 over 101s)  kubelet  Error: ErrImagePull



If above errors comes, then pull busybox image on that nodes first using : docker pull busybox

# Now delete that rabbitmq pod : 

# kubectl delete pod nfsaas-rabbitmq-1 -n sde

#kubectl get pods -n sde  (now rabbitmq should be in healthy state)

Issue 3: 

=> Authentication failure error after upgrading SDE, then you need to change in  nfsaas-configmap, change gcpPartnerService value there from cloudvolumesgcp-api.netapp.com  to  dev-cloudvolumesgcp-api.netapp.com

# kubectl edit cm -n sde nfsaas-configmap     (change gcpPartnerService value as mantioned below)

Solution: Execute "kubectl edit cm -n sde nfsaas-configmap"
then update gcpPartnerService: dev-cloudvolumesgcp-api.netapp.com

after that restart the cloud volumes service deployment : 
# kubectl rollout restart deploy cloud-volumes-service -n sde

=> After that Authentication failure erroors should be gone.

https://www.velotio.com/engineering-blog/kubernetes-python-client
*************************************************************************

The Kubernetes library comes to our aid with quite a few modules, the ones featured in this article are client and config modules from the package; we will be using these two heavily. So, lets install the Kubernetes Python Client:


root@cvs-k8s-jaynath-02:/u/jaynath/.ssh# pip show kubernetes
Name: kubernetes
Version: 11.0.0
Summary: Kubernetes python client
Home-page: https://github.com/kubernetes-client/python
Author: Kubernetes
Author-email:
License: Apache License Version 2.0
Location: /usr/local/lib/python3.6/dist-packages
Requires: certifi, google-auth, python-dateutil, pyyaml, requests, requests-oauthlib, setuptools, six, urllib3, websocket-client
Required-by:


Now that we have the python-kubernetes package installed, we can import it as:

from kubernetes import client, config

Loading cluster configurations :

config.load_kube_config()  # for local environment
# or
config.load_incluster_config()

Executing this will load the configurations for your clusters from your local or remote .kube/config file.

Interacting with Kubernetes Resources


Now that we have loaded the configurations, we can use the client module to interact with the resources.

In Python, we instantiate CoreV1Api class from client module:

v1 = client.CoreV1Api()

v1.list_node()
v1.list_namespace()

Similarly, we can list all the resources or resources in a particular namespace.

For example, to list pods in all namespaces:

v1.list_pod_for_all_namespaces()
v1.list_persistent_volume_claim_for_all_namespaces()

For all the resources that can be group within a given namespace, we can use:

# v1.list_namespaced_pod(<namespace>)
v1.list_namespaced_pod(namespace=default)

# v1.list_namespaced_service(<namespace>)
v1.list_namespaced_service(namespace=default)

We create a container using:

# container1 = client.V1Container(<name>, <image>) e.g:
container1 = client.V1Container(my_container, nginx)


3. V1PodSpec: Depending on the component we are working on, the class for its spec and params change. For a pod, we can use V1PodSpec as:

# pod_spec = client.V1PodSpec(<containers_list>)
pod_spec = client.V1PodSpec(containers=containers)

Now that we have both metadata and spec, lets construct the pods body:

pod_body = client.V1Pod(metadata=metadata, spec=pod_spec, kind='Pod', api_version='v1')


Using Python:

pod_logs = v1.read_namespaced_pod_log(<pod_name>, <namespace>)
pod_logs = v1.read_namespaced_pod_log(name=my-app, namespace=default)

#v1.delete_namespaced_pod(<pod_name>, <namespace>)
v1.delete_namespaced_pod(name=my-app, namespace=default)



Complete Example for creating a Kubernetes Pod: 



from kubernetes import client, config
	
config.load_kube_config()
v1 = client.CoreV1Api()
	
namespaces_list = v1.list_namespace()
namespaces = [item.metadata.name for item in namespaces_list.items]
	
pods_list = v1.list_namespaced_pod(namespace=default)
pods = [item.metadata.name for item in pod_list.items]

containers = []
container1 = client.V1Container(name=my-nginx-container, image=nginx)
containers.append(container1)
	
pod_spec = client.V1PodSpec(containers=containers)
pod_metadata = client.V1ObjectMeta(name=my-pod, namespace=default)

pod_body = client.V1Pod(api_version=v1, kind=Pod, metadata=pod_metadata, spec=pod_spec)
	
v1.create_namespaced_pod(namespace=default, body=pod_body)
	
pod_logs = v1.read_namespaced_pod_log(name=my-pod, namespace='default')

v1.delete_namespaced_pod(namespace=default, name=my-pod)


How to check SDE installed and do cleanup if not :
****************************************************************

https://confluence.ngage.netapp.com/display/CLOUDVOL/New+Member+Ramp-up+Plan  K8S, SDE and connect to Ontap 
https://confluence.ngage.netapp.com/display/CLOUDVOL/VP+VSIM  OpenLab VSIM creation/configuration and installation. 
https://confluence.ngage.netapp.com/display/CLOUDVOL/CVS+CIT+troubleshooting+tips

Ratom_2022.09x-6534281-non-debug

kubectl edit cm nfsaas-configmap -n sde   (change quark version as above one)
kubectl edit cm -n sde cloud-volumes-sds-configmap


1. how to check SDE installed properly
                All pods(Mysql-x, CVI, CVS, mongodb, pgbouncer, heidall, meteorqloud, drp[ANF], VPS, RT, GS), ignore if(qstack, statistics collector, telemetry upload failure, if you are not working with metrics)
                Next add ONTAP to SDE, refresh RT etc and you can crate volumes 

2. how to cleanup correct
                1. unistall kubernetes - reinstalling it
                2. cleanup up resource - PV, PVC, pods, SVC, STS, namespace, SA, deployments etc
                steps:
                sudo kubectl get svc | grep -v NAME | awk '{print "sudo kubectl delete svc "$1}' | bash
                sudo kubectl get cm | grep -v NAME | awk '{print "sudo kubectl delete cm "$1}' | bash
                sudo kubectl get secret | grep -v NAME | awk '{print "sudo kubectl delete secret "$1}' | bash
                sudo kubectl get deployment | grep -v NAME | awk '{print "sudo kubectl delete deployment "$1}' | bash
                sudo kubectl get serviceaccount | grep -v NAME | awk '{print "sudo kubectl delete serviceaccount "$1}' | bash
                sudo kubectl get pvc | grep -v NAME | awk '{print "sudo kubectl delete pvc "$1}' | bash
                sudo kubectl get job | grep -v NAME | awk '{print "sudo kubectl delete job "$1}' | bash
                sudo kubectl get pods | grep -v NAME | awk '{print "sudo kubectl delete pods --grace-period=0 --force "$1}' | bash
                kubectl delete cronjob cloud-volumes-telemetry-exporter-spooler --force --grace-period=0
                kubectl delete cronjob cloud-volumes-telemetry-exporter-anf-reporter --force --grace-period=0
                kubectl delete pvc datadir-mysql-0 force --grace-period=0

                  "kubectl get all --all-namespaces"

                  catch here to cleanup PVC's and any hung namespaces:
                                add finalizers as null and then delete them
                                ex: 
                                1> add finalizer
                                kubectl patch pvc <PVC_NAME>  -p '{"metadata":{"finalizers":null}}'  --type=merge 
                                 or
                                kubectl patch pvc <PVC_NAME> -p '{"metadata":{"finalizers": []}}' --type=merge  
                                2> Then delete
                                kubectl delete pvc data-vp-rabbitmq-0  --grace-period=0 --force    


3. how to fix the failed step/pod
                a. PVC failures - PVC not bound etc 
                                > first check if your local-path-storage namespace active(kubectl get ns)
                                > if you volume-test pod is running, if not running coredns or weavepod issue - troubleshoot look into logs/describe and fixit
                b. PVC is fine, pods are not coming-up
                                you have to delete you PVC and delete pod, PVC will comeup properly and then pods will also run fine. 
                c. rate limit issues - try again, or use local docker copy (you have to copy the image locally and then try restart the pod)
                                https://confluence.ngage.netapp.com/display/CBS/Fix+Imagepullbackoff+due+to+limit+reached
                d. rabbit-mq issues: 
                e. ONTAP issues - we have identify the issue and manually fix it, 
                                mostly disks, storage space, scrathpad issue with VSIM 

4. run sanity cases in local test bed:
                Create a jenkins job or use robot scripts to run automation from the controller machine for SSO enabled env, we have to add below steps
                Automation: SSO enabled/ passwordless SSO - creating private/public using rsa-keygen - solution for this is present. 
                                kubectl commands from automation, it will not work. 
                                mkdir-p $HOME/.kube
                                sudo cp-i /etc/kubernetes/admin.conf $HOME/.kube/config
                                sudo chown$(id-u):$(id-g) $HOME/.kube/config
                                sudo chmod 777 .kube/config
                
                Manually: 1. "sudo su" and execute commands
                                                  others to use cluster: usermod -aG sudo <sso_user>
                                                  mkdir-p $HOME/.kube
                                                sudo cp-i /etc/kubernetes/admin.conf $HOME/.kube/config
                                                sudo chown$(id-u):$(id-g) $HOME/.kube/config
                                                sudo chmod 777 .kube/config

5. fixing PVC issues - local-path-storage
                > first check if your local-path-storage namespace active(kubectl get ns)
                > if you volume-test pod is running, if not running coredns or weavepod issue - troubleshoot look into logs/describe and fixit

6. details about local-path storage for manual installation
local-path-storage:
Create default storage class on master
git clone https://github.com/rancher/local-path-provisioner.git
make below changes to local-path-provisioner/deploy/local-path-storage.yaml
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: local-path
 annotations:                                             <-- add this
      storageclass.kubernetes.io/is-default-class: "true"    <-- add this

kubectl apply -f local-path-provisioner/deploy/local-path-storage.yaml
kubectl -n local-path-storage get pod
kubectl create -f local-path-provisioner/examples/pvc/pvc.yaml
kubectl create -f local-path-provisioner/examples/pod/pod.yaml
kubectl get pv
kubectl get pvc

Helpful links:
kubernetes SDE installation guide: https://confluence.ngage.netapp.com/display/CLOUDVOL/New+Member+Ramp-up+Plan
troubleshooting guide: https://confluence.ngage.netapp.com/display/CLOUDVOL/Internal+Troubleshooting+Guide+CVS+QA
mysql pod issues: https://confluence.ngage.netapp.com/display/ANF/MySQL+disaster+recovery+when+mysql-0+is+unable+to+start

Some Helpful kubernetes commands:
Pod related:
1. kubectl get pods -n <namespace> 
 or kubectl get pods (if default namespace)
2. kubectl get pods | grep <podname>
3. kubectl describe pod <podname> -n namespace 
or kubectl describe pod <podname>  (if default namespace)
4. kubectl logs pod -n namespace 
or kubectl logs pod (if default namespace)
5. kubectl exec -it <podname> sh
exp: kubectl exec -it mysql-0 sh
6. kubectl get nodes -o wide   (with ip and label details of nodes)
7. kubectl get all --all-namepaces
8. kubectl get cm <configmapname>
9. kubectl get deployments
10. kubectl edit deployment <deploymentname>
11. kubectl get sts  - statefulsets
12. kubectl edit sts <stsname>
13. kubectl get svc -n namespace
or kubectl get svc   (if default namespace)
14. kubectl get svc | grep <svcname>
15. kubectl scale deployment --replicas=x -n namespace
or  kubectl scale deployment --replicas=x   (if default namespace)
16. kubectl get pv
17. kubectl delete pod <podname>
18. kubectl delete pvc <pvcname>
19. kubectl delte pv <pvname>
20. kubectl expose pod <podname> -n namespace --type=NodePort   - for exposing a pod as NodePort
or   kubectl expose pod <podname> --type=NodePort  (if default namespace)
Reply if you need more information.


Kubernetes networking services :
***************************************
Kubernetes Networking, Services, NodePort & Volumes :

=> Containers within a pod use networking to communicate via loopback

=> Cluster Networking provides communication between different pods

=> The service resources lets you expose on application running in pods to be reachable from outside your cluster

=> You can also use services to publish services only for consumption inside your cluster

=> Container to container communication on same pod happens through localhost within the containers


Scenario 1: How 2 containers within same pod communicates :

# Container to container communication within same pod:

# vi pod.yaml

kind: pod
apiVersion: v1
metadata:
  name: testpod
spec:
  containers:
    - name: c00
      image: ubuntu
      command: ["/bin/bash", "-c", "while true;do echo Jaynath Kumar;sleep 5s; done"]
    - name: c01
      image: httpd
      ports:
        - containerPort: 80

root@cvs-k8s-jaynath-01:/jaynath_dummy# kubectl apply -f pod.yaml
pod/testpod created


root@cvs-k8s-jaynath-01:/jaynath_dummy# kubectl get pods
NAME            READY   STATUS    RESTARTS   AGE
testpod         2/2     Running   0          12s

=> Login inside container c00 and try to access container c01 through localhost and port number
root@cvs-k8s-jaynath-01:/jaynath_dummy# kubectl exec -it testpod bash -c c00
kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.
root@testpod:/# curl http://localhost:80
<html><body><h1>It works!</h1></body></html>
root@testpod:/#

Scenario 2:  Now try to establish communication between 2 different pods within same machine

=> Pod to pod communication on same worker node happens through pod ip

=> By default pods IP will not be accessible outside the node.

root@cvs-k8s-jaynath-01:/jaynath_dummy# kubectl get pods -o wide
NAME            READY   STATUS    RESTARTS   AGE     IP             NODE                 NOMINATED NODE   READINESS GATES
testpod1        1/1     Running   0          10m     10.244.5.9     cvs-k8s-jaynath-03   <none>           <none>
testpod2        1/1     Running   0          3m8s    10.244.5.10    cvs-k8s-jaynath-03   <none>           <none>


Object Services :
-----------------------
=> Each pod gets it's own ip address, however in a deployment, the set of pods running in one moment in time could be different from the set of pod running that application a moment later.

=> Service object is an logical bridge between pods and end users which provided virtual IP


Although each pod has a unique ip address, those IP's are not exposed outside the cluster

Services helps to expose the VIP mapped to the pods & allows application to receive traffic

Creating a servie will create endpoint to access the pods/application in it.



Services can be exposed in different ways by specifying a type in the service spec:

-> Cluster IP
-> Node Port
-> LoadBalancer : created by cloud providers that will route external traffic to every node on the nodeport (e.g ELB or AWS)

NodePort service can run between nodeport : 30,000 - 32767

Headless -> creates several endpoints that are used to produce DNS records, each dns record is bound to a pod.

=> If pod1 of node1 wants to establish communication with pod2 of node2, then you need to use ClusterIP port

ClusterIp will be mapped to each node of your cluster





